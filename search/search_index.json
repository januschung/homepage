{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#hi-my-name-is-janus-chung","title":"Hi, My Name is Janus Chung","text":"<p>  I am a Software Engineer with over 20 years of experiences in various fields (Developer, DevOps, QA, Support).</p> <p>Currently, I work as a Senior Software Engineer DevOps for Ad Hoc, which is a digital services company that helps the federal government better serve people. My day to day work involves setting up Kubernetes (EKS) and other AWS services as a plaform for various Veterans Affairs services.</p> <p>Before joining Ad Hoc, I had been working as a Staff Software Engineer for over seven years with Integral Ad Sciences, which is a global leader in digital media quality. IAS makes every impression count, ensuring that ads are viewable by real people, in safe and suitable environments, activating contextual targeting, and driving supply path optimization.</p>"},{"location":"#resume","title":"Resume","text":"<p> Work Experience  Resume</p>"},{"location":"#professional-skills","title":"Professional Skills","text":"Web/API Development <ul> <li> Java <ul> <li>Springboot </li> <li>Reactive</li> </ul> </li> <li> Python<ul> <li> Fastapi</li> <li> Flask</li> </ul> </li> <li> PHP <ul> <li> Laravel</li> <li> CakePHP</li> </ul> </li> <li> JavaScript <ul> <li> Node</li> <li> React</li> </ul> </li> <li> Go</li> <li> MySQL,  Postgres,  DynamoDB</li> <li>REST,  GraphQL</li> </ul> DevOps <ul> <li> Terraform</li> <li> AWS<ul> <li>CloudFormation, EKS, EC2, IAM, VPC, ECR, ECS, Fargate, MSK, KMS, Lambda, RDS, CloudWatch</li> </ul> </li> <li> Jenkins</li> <li> Github Actions</li> <li> Docker</li> <li> Kubernetes</li> <li> Argo CD, Argo Rollouts</li> <li> DataDog</li> <li> Auth0</li> <li> Vault</li> </ul> QA <ul> <li>JUnit, Mockito</li> <li>TDD, Cucumber</li> <li> Pytest</li> <li> Selenium WebDriver</li> </ul>"},{"location":"#what-i-like","title":"What I like","text":"Programming <p>I love building stuffs and learning new technology. I build open source projects with what I learned from time to time. Checkout the Opensource projects section.</p> Beer <p> </p> <p>One of my life goals is to drink as many type of beer as possible. I might have tried over 500 different beers so far. Besides drinking, I am also a beer glass collector.</p> Photography <p> </p> <p>I used to take a lot of pictures with my Sony Alpha A900. During that period of time, people portrait and wedding documentary were my favorite. It was my honor to help two of my best friends and one of my primary school classmate to capture their big moments. I will upload some of my work in the coming future.</p> Hi-Fi <p> </p> <p>The first time I listened to a real Hi-End Hi-Fi, I knew it my life would no longer be the same.  Thanks to my late Uncle 17 who brought me to the Hi-Fi world when I was 12. My current setup is DM602 S2 with a Mid range Yamaha AV Receiver.</p> Reading <p> </p> <p>Inspired by my secondary school class teacher, who told me his story about reading latest books from the bookstore when he could not afford it during his adolesencent age , I picked up reading as a hobby. My reading record is 52 books in a year.</p> Woodworking <p> </p> <p>I built couple furnitures for my home with the skills I learned from secondary school D&amp;T class, as well as from a summer job in a double deck bus repairing factory. Working with wood always gives me a peaceful feeling.</p>"},{"location":"about-me/about-this-homepage/","title":"About this Homepage","text":"<p>While chatting with my ex-coworker and buddy about job hunting and project, we came up with an idea to build a personal homepage since a lot of job application is asking for it.</p> <p>I think it is a good idea since it is a good way to showcase not only the skillset and work experience in a more presentable way, compare to boring pdf/word document, but also provides more content about how we are outside of work.</p>"},{"location":"about-me/about-this-homepage/#mkdocs-material","title":"Mkdocs Material","text":"<p>From the new trick that I learned in my new job, I built this personal homepage once again (the last time was a school project using plain HTML and javascript). It uses Mkdocs Material which provides a simple but yet professional theme that works well in any devices with built in responsiveness. It is easy to use without any coding skill whatsoever, as it just requires some light modification of a configuration yaml file and adding/editing markdown file(s) for your contents.</p>"},{"location":"about-me/about-this-homepage/#free-publishing-and-hosting-with-github-page","title":"Free Publishing and Hosting with Github Page","text":"<p>Another good thing is, not only the source code of the homepage can be hosted on Github, but also the web hosting is provided for free with Github Pages. Powered with Github Action, Github will build and deploy the homepage automatically. The process is fully automated so that the next time you when add or edit your repo, the change will be deployed within a minute without further work required.</p> <p>I hope this can help my buddy to build a good porfolio, as well as to get a dream job soon in this troublesome tech job market. If anyone of you is interested in learning more, feel free to contact and I am happy to help.</p>"},{"location":"about-me/about-this-homepage/#further-readings","title":"Further readings","text":"<p>Mkdocs Material</p> <p>Github Action to publish your site</p> <p>Github Pages</p>"},{"location":"about-me/certificate/","title":"Certificate","text":""},{"location":"about-me/certificate/#learn-devops-infrastructure-automation-with-terraform","title":"Learn DevOps: Infrastructure Automation With Terraform","text":"<ul> <li>Terraform course material</li> </ul>"},{"location":"about-me/certificate/#the-complete-github-actions-workflows-guide","title":"The Complete GitHub Actions &amp; Workflows Guide","text":"<ul> <li>GitHub Documentation</li> <li>Simple Docke NodeJs API repo</li> <li>Send Slack Message with Docker</li> <li>Surge - Service to deploy and test static website</li> <li>Semantic Versioning</li> </ul>"},{"location":"about-me/certificate/#argo-cd-essential-guide-for-end-users-with-practice","title":"Argo CD Essential Guide for End Users with Practice","text":"<ul> <li>ArgoCD GitHub Repo</li> <li>Example App</li> </ul>"},{"location":"about-me/certificate/#build-reactive-microservices-using-spring-webfluxspringboot","title":"Build Reactive MicroServices using Spring WebFlux/SpringBoot","text":"<ul> <li>Course GitHub Repo</li> </ul>"},{"location":"about-me/certificate/#become-a-superlearner-2-learn-speed-reading-boost-memory","title":"Become a SuperLearner\u00ae 2: Learn Speed Reading &amp; Boost Memory","text":""},{"location":"about-me/certificate/#apache-kafka-series-learn-apache-kafka-for-beginners-v3","title":"Apache Kafka Series - Learn Apache Kafka for Beginners v3","text":"<ul> <li> <p>Kafka basics from Conduktor.io</p> </li> <li> <p>Sample Code</p> </li> </ul>"},{"location":"about-me/certificate/#web-developer-bootcamp-with-flask-and-python","title":"Web Developer Bootcamp with Flask and Python","text":"<ul> <li>Github repo for Habbit Tracker Project</li> <li>Github repo for Profolio Project</li> <li>Github repo for Movie Watchlist Project</li> </ul>"},{"location":"about-me/certificate/#go-the-complete-developers-guide-golang","title":"Go: The Complete Developer's Guide (Golang)","text":""},{"location":"about-me/certificate/#the-complete-ruby-on-rails-developer-course","title":"The Complete Ruby on Rails Developer Course","text":""},{"location":"about-me/certificate/#istio-hands-on-for-kubernetes","title":"Istio Hands-On for Kubernetes","text":""},{"location":"about-me/hackathon-champion/","title":"Hackathon Champion","text":"<p>It was one of my best achievements in career to win the company hackathon leading a team of five engineers from different teams.</p> <p></p>"},{"location":"about-me/hackathon-champion/#background","title":"Background","text":"<p>It began with the time when I was bored working as a QA Automation Engineer in the company for 4 years and I would like to do more outside of my conform zone.</p> <p>I was bored</p> <p>Working as a QA Automation Engineer with day to day repetitive tasks was very boring. I was looking for new challenge.</p> <p>In a quarterly PI planning, when all of the engineers around the world came to the NYC office, I told my team lead that I wanted to install our flagship platform on my laptop as a local development environment so that I could learn more about how it worked. He pointed me to a Confluence page with instruction that took 20 scrolls to reach to the bottom.</p>"},{"location":"about-me/hackathon-champion/#prerequisite","title":"Prerequisite","text":"<ol> <li>Java</li> <li>Web container</li> <li>Database</li> <li>Cache server</li> <li>Five internal repo</li> </ol> <p>After that came with lengthy instruction to setup configurtation for various servers and repo</p>"},{"location":"about-me/hackathon-champion/#configuration","title":"Configuration","text":"<ol> <li>Web container configuration</li> <li>Java Spring properties</li> <li>Application configuration</li> <li>Database configuration</li> <li>Script to prepopulate cache</li> <li>download third party files</li> <li>10+ more things</li> </ol> <p>A whole week to setup</p> <p>It took an average of a week to setup the monolith for new hired. New hires were asking for help in the support slack channel desperately.</p> <p>That was a tedious process and it took me 2 days to set it up! During the process, I had to consult with three different senior developers. It turned out that I found at least two outdated instructions from the Confluence page.</p> <p>Not ideal.</p> <p>From that point, I spent a week of free time trying to simplify the installation. I did not want anyone to suffer anymore, especially for the new hires. I tried to Dockerize the monolith plaform. However, I failed due to my lack of domain knowledge from a development standpoint. Besides, my Docker skill was still green.</p>"},{"location":"about-me/hackathon-champion/#hackathon","title":"Hackathon","text":"<pre><code>flowchart RL\n  subgraph Web Container Image\n  F{{config a}} --&gt; A\n  G{{config b}} --&gt; A\n  H{{config c}} --&gt; A\n  war([Monolith artifact]) --&gt; A\n  js([js artifact]) --&gt; A\n  end\n  A(((Web Container))) --&gt; B[(Database)]\n  subgraph Cache Service Image\n  A --&gt; C[(Cache Server)]\n  D[Cache Warming Job] --&gt; C\n  end\n  subgraph Database Image\n  E[Liquibase/Flyway] --&gt; B\n  J{{DB Setup Script}} --&gt; B\n  end</code></pre> <p>Two years later, after I transferred into a development and devops role, the Hackathon came. While brainstorming with a devops buddy for potential team idea, he challenged me to Dockerize the monolith platform. That reignited my hidden burning desire to give it another try.</p>"},{"location":"about-me/hackathon-champion/#easy-first-draft","title":"Easy First Draft","text":"<p>At first, I thought it would be hard since I failed two years ago. However, it turned out to be a 2 hours easy work to build the first working draft!</p> <p>When I think back now, here were the reasons:</p> <ol> <li> <p>The database had already been containerized in those two years. It was a team effort between another developer and me with our free time.</p> </li> <li> <p>I gained more domain knowledge of the monolith plaform. What didn't make sense from the server and application logs in the past became meaningful during the troubleshooting process.</p> </li> <li> <p>I was more familiar with most of the configuration files and fields since I had been working on the codebase for the past two years.</p> </li> <li> <p>My Docker skill had grown so much from the work as well as from the home server setup that I did during my free time.</p> </li> </ol> <p>The first draft was working with the following setup:</p> <ol> <li>Database in a Docker</li> <li>Local cache server with warm up script ran manually</li> <li>Monolith Java war file in a Docker web container with required configuration files</li> <li>Monolith was setup to wire with the database in Docker and the local cache server</li> </ol> <p>It worked! Since I had one and half day left, I spent the rest of the time to automate everything together. My plan was to have the monolith be brought up with a single command.</p>"},{"location":"about-me/hackathon-champion/#final-draft","title":"Final Draft","text":"<p>I continuted to Dockerize other components, including the cache server and its cache warm up process. I also made the new repo as the single source of the truth for all the configuration files. Here is the final docker-compose file (with some naming modification):</p> <pre><code>version: '2.3'\n\nnetworks:\n  internal:\n    driver: bridge\n    ipam:\n      driver: default\n      config:\n        - subnet: 172.16.238.0/24\n\nservices:\n  service-a:\n    image: the-cache-server\n    ports:\n      - \"22122:22122\"\n    networks:\n      internal:\n        ipv4_address: 172.16.238.2\n  db:\n    image: internal-cr/the-db-image:latest\n    init: true\n    ports:\n      - \"1234:1234\"\n    networks:\n      internal:\n        ipv4_address: 172.16.238.3\n  monolith:\n    image: internal-cr/monolith:latest\n    ports:\n      - \"9090:9090\"\n      - \"9091:9091\"\n    depends_on:\n      - service-a\n      - db\n    volumes:\n      - ./etc/webcontainer/:/etc/webcontainer/\n      - ./artifact/monolith/:/usr/local/webcontainer/monolith/\n      - ./artifact/monolith-js:/var/data/deploy/monolith-js\n      - ./var/data/conf:/var/data/conf\n      - ./config:/var/data/config\n    hostname: monolith\n    networks:\n      internal:\n        ipv4_address: 172.16.238.4\n</code></pre> <p>Compared to following a 10 pages Confluence page and two days to setup a local development environment for the monolith, the process became a simple 4 steps task:</p> <ol> <li>Build and drop the Java war file to the <code>artifact/monolith</code> directory</li> <li>Get the latest monolith-js artifact and drop it to the <code>artifact/monolith-js</code> directory</li> <li>Build the Docker image</li> <li>Bring up the monolith with <code>docker-compose up</code></li> </ol>"},{"location":"about-me/hackathon-champion/#follow-up-meeting","title":"Follow Up Meeting","text":"<p> At that point, I didn't really care much if I would win the hackathon since I've already achieved something that I couldn't do in the past. I also believed that the project will help everyone in the future to get onboard within an hour or two compares to days of tedious manual setup.</p> <p>However, I understood that if my voice was not being heard, all the effort would just stay as a dead project without really helping anyone.</p> <p>For that, I made a bold move to call off a meeting with all the related party, plus some big guys:</p> <ul> <li>Director and Manager of SysOps</li> <li>Director of DevOps</li> <li>Managers and Tech Leads from two teams</li> <li>CTO, VP of Engineering</li> </ul> <p>In an hour of meeting, besides descring what the project has solved and the reason behind it, I brought up the following that the project could potentially help:</p> <ul> <li>To migrate the CICD process of the Dockerized monolith application to Kubernetes </li> <li>To run acceptance test and regression in parallel faster with horizontal scaling</li> <li>To test web container, Java and database update easily</li> </ul> <p>Suprisingly, these were some of the problem which they would like to solve on their roadmap or backlog for the next two years. We ended up creating epics for respective team to further develop the project. It was a big success.</p>"},{"location":"about-me/hackathon-champion/#hackathon-result","title":"Hackathon Result","text":"<p>It still feel so surreal and overwhelming to win the hackathon for the company. The most valuable part was that I learned to step up and organized the meetings with the right people to drive cross team collaboratoin and further develop the project. I also gained the trust within the Engineering and Product teams and built great connection and friendship with the team members.</p>"},{"location":"about-me/work-experience/","title":"Work experience","text":"AD HOC LLC  WASHINGTON, DC (Remotely Based) INTEGRAL AD SCIENCE (IAS)  NEW YORK, NY (Remotely Based) IV INTERACTIVE  JERERY CITY, NJ MEDIA THREE CORPORATION  NEW YORK, NY L &amp; L TRAVEL ENTERPRISE, INC  NEW YORK, NY MERCK &amp; CO., INC  MORRISTOWN, NJ DATASYNAPSE, INC  NEW YORK, NY APPLIMATION, INC  NEW YORK, NY"},{"location":"about-me/work-experience/#ad-hoc-llc","title":"AD HOC LLC","text":"<p> SENIOR SOFTWARE ENGINEER DEVOPS  Feb 2023 - Present </p> <ul> <li>Built and maintained CI/CD pipelines using Github Actions, Argo CD, DataDog and Hashicorp Vault.</li> <li>Built and deployed Infrastructure as Code using Terraform on AWS EKS.</li> <li>Created standardized platform EKS service to get Veterans Affairs API services onboard to AWS.</li> </ul>"},{"location":"about-me/work-experience/#integral-ad-science","title":"INTEGRAL AD SCIENCE","text":"<p> STAFF SOFTWARE ENGINEER  Jun 2020 - Dec 2022 </p> <ul> <li>Built multiple microservices to migrate legacy monolith applications over to Kubernetes in AWS EKS which made feature updates faster and improved developer experience.</li> <li>Designed and developed APIs for IAS pre-bid and post-bid platform using Spring boot with a MySQL backend running in Tomcat. This project enabled IAS\u2019 clients to integrate with our targeting segments quickly.</li> <li>Created a new microservice in AWS via ECS/Fargate. It was called Reporting API and it allowed clients to request reports programmatically.</li> <li>Worked on integrating systems developed by the acquired company (Admantx). We had to receive their data and incorporate it as part of our pre-pid feature set.</li> <li>Created multiple mock services with Spring Boot and Cucumber framework to improve development in non-production environment and enhance automated tests.</li> <li>Mentored and helped new engineers integrate in IAS as part of the co-pilot program.</li> <li>Created automated test-suites in Python for regression and acceptance tests.</li> <li>Dockerized an existing monolith Spring application to improve developer experience. This project won in the company hackathon.</li> <li>Conducted bi-weekly Lunch and Learn sessions for the team to present and discuss new technologies, and brainstorm improvement for our current tech stack.</li> </ul> <p> STAFF QAA ENGINEER  Jun 2015 - May 2020 </p> <ul> <li>Created headless XVFB and Chrome/Firefox test framework to speed up testing time by 80%.</li> <li>Created a reusable testing environment for different products using Puppet that spins up AWS EC2 environments on demand.</li> <li>Refactored legacy regression suite and cut testing time from 90 to 25 minutes.</li> </ul>"},{"location":"about-me/work-experience/#iv-interactive","title":"IV INTERACTIVE","text":"<p> FULL STACK SOFTWARE ENGINEER  Aug 2014 - Jun 2015 </p> <ul> <li>Designed and created a HIPPA compliant Web Application for IV Interactive using Laravel(PHP framework), MySQL, jQuery, Bootstrap, Skeleton, SASS/CSS3 and HTML5.</li> <li>Created and maintained unit tests for the above using PHPUnit.</li> <li>Proposed and implemented secure backups of codebase and database with PGP data encryption.</li> </ul>"},{"location":"about-me/work-experience/#media-three-corporation","title":"MEDIA THREE CORPORATION","text":"<p> FULL STACK SOFTWARE ENGINEER  Jun 2013 - Aug 2014 </p> <ul> <li>Implemented customer support system using CakePHP(PHP framework), MySQL, JQuery, CSS3. This was used to monitor customer complaints and inquiries.</li> </ul>"},{"location":"about-me/work-experience/#l-l-travel-enterprise-inc","title":"L &amp; L TRAVEL ENTERPRISE, INC","text":"<p> SENIOR SOFTWARE ENGINEER DEVOPS  Aug 2012 - Jun 2013 </p> <ul> <li>Created and maintained the Online booking/tracking system for 500 travel agents using IIS, ASP, Javascript and SQL Server.</li> </ul>"},{"location":"about-me/work-experience/#merck-co-inc","title":"MERCK &amp; CO., INC","text":"<p> INFORMATICA DEVELOPER (Consultant)  Jun Jun 2011 - Feb 2012 </p> <ul> <li>Worked on adding features and upgraded Informatica from version 4.2 to 5.3</li> </ul>"},{"location":"about-me/work-experience/#datasynapse-inc","title":"DATASYNAPSE, INC","text":"<p> SENIOR SUPPORT ENGINEER  May 2007 - Nov 2009 </p> <ul> <li>Provided technical support for customers using DataSynapse software, a distributed computing application, on their mission-critical production systems in various platforms.</li> <li>Developed sample code/examples of the DataSynapse API to make integrations for clients easier.</li> </ul>"},{"location":"about-me/work-experience/#applimation-inc","title":"APPLIMATION, INC","text":"<p> INFORMIA SUPPORT CONSULTANT  May 2005 - Apr 2007 </p> <ul> <li>Provided technical support for customers using Applimation software.</li> <li>Developed custom Oracle/PeopleSoft/Siebel SQL metadata for customers.</li> </ul>"},{"location":"home-server/about/","title":"How to apply fun technologies you learn at work for home","text":""},{"location":"home-server/about/#originally-posted-at-ias-medium-blog-at-dec-17-2020","title":"originally posted at IAS medium blog at Dec 17, 2020","text":"<p>I love technology and I am lucky enough to get paid for pursuing this hobby. So it is no surprise that in my spare time at home I am experimenting with new technologies and leveraging those to make my life at home a bit more convenient. One of the latest projects I worked on at home was rebuilding my home server.</p> <p>In this new socially distant reality, the tools I use at work (Docker and automation with Git and Jenkins) have helped me build a home server to unify and simplify entertainment, and connect with family and friends via:</p> <ul> <li>central media server to stream music and video</li> <li>personal cloud to backup photos</li> <li>firewall for network security</li> <li>family blog</li> </ul>"},{"location":"home-server/about/#docker-and-docker-compose","title":"Docker and Docker Compose","text":"<p>At IAS, we use Docker, from development to deployment, both leveraging official images or building our own. Since Docker is so helpful, I decided to use it on my own server. I found the following benefits with this move:</p> <ul> <li>maintainable (each Docker component is scripted in a docker-compose.yml file)</li> <li>upgradable with ease (rebuild the service with the latest image)</li> <li>easy to organize and backup (all persistence Docker volumes live in their own directory)</li> <li>migratable (though my server crashed months ago, it was easy to rebuild everything on a new virtual machine)</li> </ul>"},{"location":"home-server/about/#automation-with-git-and-jenkins","title":"Automation with Git and Jenkins","text":"<p>Automation is the new mantra in our day-to-day work on the IAS Prime team, with Git and Jenkins playing a big role in that mindset. We automate spinning up environments, running tests, creating bug reports from Slack messages, deploying software updates, and AWS Autoscaling.</p> <p>My main take-away from working on the AWS onboarding project with the Jenkins pipeline is how convenient it is to set up CICD with code. I was amazed by the results, so I borrowed the same approach for home. Now, I never have to log in to the server to run scripts. With the Jenkins web interface, I can review the job status and reports whenever needed. I run repeatable tasks in my home server with the following:</p> <ul> <li>Jenkins (in a Docker)</li> <li>Git (Gitea, a lightweight git server, in a Docker)</li> </ul> <p>Then I create shell scripts to handle these repeatable maintenance tasks:</p> <ul> <li>auto-deployment of Docker image (by checking in a new docker-compose.yml file to Gitea which wired to Jenkins for the deployment)</li> <li>weekly backup of data (Jenkins Scheduler to pull the back up script from Gitea)</li> <li>weekly upgrade of docker images (Jenkins Scheduler to pull the upgrade script from Gitea)</li> </ul> <p>I had a lot of fun rebuilding the server and it was a reminder of what I have learned from my daily work. It\u2019s also a little showcase to my non-techy family members of what I do on the job.</p> <p>Learning and using technology goes both ways. On weekends, I use an open-source Python project that I created to generate math quiz sheets for family and friends, which in turn have improved my Python skills for work.</p> <p>Integrating technology into my family life reminded me about why I got into this field \u2014 I have a passion for technology. Sometimes I ask myself where all this learning has benefitted me most\u2026 is what I\u2019m learning at work helping me at home, or is what I\u2019m learning at home helping me at work?</p> <p>At Integral Ad Science I love that my job allows for experimentation and learning new technologies. I am proud of the systems we have built over the last few years and I continue to be amazed by the volumes being processed (3B \u2014 5B events per hour) through those systems. I love the team I am working with and feel that I grow as a developer.</p> <p>At home I love playing with computer hardwares. My new hobby is setting up thin client machines around the house since they are low power (15 W) and silent (fan-less). I use them as a media streamer, for light web browsing and watching O\u2019Reilly training video courses (a benefit of working in the IAS).</p> <p>In the end, it doesn\u2019t matter because it\u2019s the technology that drives my happiness in my work and my personal life.</p>"},{"location":"home-server/hardware/","title":"Hardware","text":""},{"location":"home-server/hardware/#servers","title":"Servers","text":"<p> HP Microserver Gen 8 (A)</p> Specs <ul> <li>Xeon E3-1265L V2 (4 Cores 8 Threads)</li> <li>16GB ram</li> </ul> OS <ul> <li>Esxi</li> <li>Ubuntu</li> </ul> Usages <ul> <li>OpenMediaVault (NAS)</li> <li>Plex/JellyFin</li> <li>Gitea</li> <li>Jenkins</li> <li>lighttpd (web server)</li> </ul> <p> HP Microserver Gen 8 (B)</p> Specs <ul> <li>Xeon E3-1265L V2 (4 Cores 8 Threads)</li> <li>16GB ram</li> </ul> OS <ul> <li>Proxmox</li> </ul> Usages (TBD) <ul> <li>Kind</li> <li>ArgoCD</li> <li>Packer</li> </ul>"},{"location":"home-server/hardware/#thin-clients","title":"Thin Clients","text":"<p> HP T610</p> Specs <ul> <li>AMD Dual-Core T56N APU with Radeon HD 6320 Graphics (1.65 GHz, 1MB L2 cache)</li> <li>8GB ram</li> </ul> OS <p>Puppy</p> Usage <ul> <li>Standalone reading station</li> <li>Music client connecting to a JVC FS-2000 mini Hi-Fi</li> </ul> <p> HP T620 (x2)</p> Specs <ul> <li>AMD GX-415GA Quad-Core APU with AMD Radeon HD 8330E (1.5 GHz)</li> <li>8GB ram</li> </ul> OS <p>Linux Lite</p> Usage <ul> <li>Media Server connecting to TV</li> <li>Retro game emulator with Retropie</li> </ul>"},{"location":"home-server/kubernetes-lab/","title":"Note to build k8s homelab","text":""},{"location":"home-server/kubernetes-lab/#install-minikube-on-ubuntu","title":"Install MiniKube on Ubuntu","text":"<pre><code>curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube_latest_amd64.deb\nsudo dpkg -i minikube_latest_amd64.deb\n\n\nsudo groupadd docker\nsudo usermod -aG docker $USER\nnewgrp \nsudo chmod 666 /var/run/docker.sock\n\nminikube start\n</code></pre>"},{"location":"home-server/kubernetes-lab/#deploy-hello-minikube-and-expose-it-remotely","title":"Deploy hello-minikube and expose it remotely","text":"<p><pre><code>kubectl create deployment hello-minikube --image=kicbase/echo-server:1.0\nkubectl expose deployment hello-minikube --type=NodePort --port=8080\nkubectl port-forward --address 0.0.0.0 service/hello-minikube 8080:8080\nkubectl port-forward service/hello-minikube 8080:8080 # this will only expose it within the host\n</code></pre> Figured out I cannot connect to minikube app remotely easily. Time to switch to k3s.</p>"},{"location":"home-server/kubernetes-lab/#install-k3s-on-ubuntu","title":"Install K3s on Ubuntu","text":"<pre><code>curl -sfL https://get.k3s.io | sh -\nsudo chmod 644 /etc/rancher/k3s/k3s.yaml\nexport KUBECONFIG=/etc/rancher/k3s/k3s.yaml\n</code></pre> <p>Create sample app that can be accessed within LAN</p> <pre><code>kubectl create deployment hello-world --image=kicbase/echo-server:1.0\nkubectl expose deployment hello-world --type=NodePort --port=8080\n</code></pre> <p>install argocd </p> <pre><code>export ARGOCD_VERSION=v2.10.0\nkubectl create namespace argocd\nkubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/$ARGOCD_VERSION/manifests/core-install.yaml\nkubectl delete -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/$ARGOCD_VERSION/manifests/core-install.yaml\n\nkubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/$ARGOCD_VERSION/manifests/install.yaml\n</code></pre>"},{"location":"home-server/operating-system/","title":"Operating system","text":"<p>I started using linux at home since Ubuntu 6.04 and never look back to Windows since then. Ubuntu, Kubuntu, Centos, Linux Lite, Mint, Puppy are some of my favorites. </p> <p>Currently I am using the followings:</p> Ubuntu <p></p> <p>As the server of my docker farm.</p> Kubuntu <p></p> <ul> <li>For my spare desktop setup</li> <li>For my wife's laptop for daily web browsering and video streaming</li> </ul> Linux Lite <p></p> <ul> <li>As the media streaming boxes in living room and bedroom connecting to the TVs</li> <li>For my children' laptops which they use it for school work and light gaming</li> </ul> Puppy <p></p> <ul> <li>Deployed on a HP T610 thin client as a ebook reading station connecting to a monitor in portrait mode</li> <li>Double as a music client connecting to a vintage JVC FS-2000 mini Hi-Fi</li> </ul> Retropie/Batocera <p> </p> <p>I love playing retro games and have been sharing the love with my kids.</p>"},{"location":"home-server/recovery/","title":"Steps to troubleshoot when home server is down after power loss","text":"<ol> <li> <p>Connect a laptop to the router and check for internet connection</p> </li> <li> <p>Visit the ESXi UI to see if the server is up</p> </li> <li> <p>If ther server is not up </p> <ul> <li>Connect the server with monitor, keyboard and network cable</li> <li>Power on/Force restart of the server</li> </ul> </li> <li> <p>Start up the Firewall and PiHole VMs</p> </li> <li> <p>Check server 2 ESXi UI and restart it if needed</p> </li> <li> <p>Start up VM</p> </li> <li> <p>Bring up all docker services</p> </li> </ol>"},{"location":"home-server/software/","title":"Software","text":""},{"location":"home-server/software/#pfsense","title":"Pfsense","text":"<p>Besides serving as a fireawll, it is used to seperate my main network from the IoT network. I also use it to limit internet access for my kids.</p>"},{"location":"home-server/software/#pihole","title":"PiHole","text":"<p>A must have to block ad servers. Pairing with Pfsense as the default DNS, all of my home devices are filtered from 90% of unwanted ad.</p>"},{"location":"home-server/software/#openmediavault","title":"OpenMediaVault","text":"<p>A free NAS also serving as a NFS server.</p>"},{"location":"home-server/software/#jellyfin","title":"Jellyfin","text":"<p>A free media server and an alternative of Plex. It supports playing music and video with its android app out of the box and it is free of unwanted ad/extra services. With that, I can stream music/movie from any device, as well as TV, within my home network.</p>"},{"location":"home-server/software/#lighttpd","title":"Lighttpd","text":"<p>A very light weight web server. It is used to send ebooks to my Kindles via the experimental web browser.</p>"},{"location":"home-server/software/#gitea","title":"Gitea","text":"<p>A light weight git server to store private repo, home server setup scripts, etc.</p>"},{"location":"home-server/software/#jenkins","title":"Jenkins","text":"<p>A CICD server to handle home server deployment and to run maintenance scripts on schedule.</p>"},{"location":"opensource-projects/asian-character-worksheet-generator/","title":"Asian Character Worksheet Generator","text":""},{"location":"opensource-projects/asian-character-worksheet-generator/#background","title":"Background","text":"<p>As an immigrant and father of two boys living in America, I want my sons to learn my language so that they can have the chance to learn more about their culture origin. I want to share with them the novels that I have read; the music that I have listened to and the movies that I have watched. To make it happens, it is essential to teach them how to write in my language.</p> <p>At first I started making worksheet by using Google Doc. I had to create the layout manually (in grid format) and adjust the spacing. Then I had to type in character by character to each cell and readjust the spacing one by one. This project saves you time from the tedious manual effort of doing a copy and paste of a new worksheet. Also, this project automatically does all the work to adjust the column widths which is frankly not that enjoyable to do manually (Hey I want my beer time).</p> <p>That's the reason for me to look into an automate way to get the job done.</p>"},{"location":"opensource-projects/asian-character-worksheet-generator/#benefits-of-grid-system","title":"Benefits of Grid System","text":"<p>The grid system helps someone learning the language to write the characters in the proper size and orientation. The grid system gives a worksheet for the new learner to practice writing the characters.</p>"},{"location":"opensource-projects/asian-character-worksheet-generator/#requirements","title":"Requirements","text":"<ol> <li>python3</li> <li>fpdf</li> </ol>"},{"location":"opensource-projects/asian-character-worksheet-generator/#how-to-use","title":"How to Use","text":"<ol> <li>Edit the file <code>text</code> with the list of characters of your choice (or use the sample). Note that you need to supply 14 characters on each line. <pre><code>\u4e00\u4e8c\u4e09\u56db\u4e94\u516d\u4e03\u516b\u4e5d\u5341\u767e\u5343\u5de6\u53f3\n\u4e0a\u5927\u4eba\u5b54\u4e59\u5df1\u5316\u4e09\u5343\u4e03\u5341\u58eb\u5973\u5c0f\n\u751f\u516b\u4e5d\u5b50\u4f73\u4f5c\u4ec1\u53ef\u77e5\u79ae\u4e5f\u4f60\u6211\u4ed6\n\u7236\u6bcd\u5144\u5f1f\u4e0a\u4e2d\u4e0b\u6771\u5357\u897f\u5317\u541b\u89aa\u5e2b\n\u65e5\u6708\u91d1\u6728\u6c34\u706b\u571f\u5929\u5730\u6d77\u5b87\u5b99\u661f\u5bbf\n\u76ee\u8033\u9f3b\u53e3\u7709\u624b\u8db3\u820c\u76ae\u5fc3\u808c\u809d\u80ba\u8178\n\u3042\u3044\u3046\u3048\u304a\u304b\u304d\u304f\u3051\u3053\u30a2\u30a4\u30a6\u30a8\n</code></pre></li> <li>Generate the worksheet in pdf format with the following command: <pre><code>python3 run.py\n</code></pre></li> <li>Print out the generated file <code>worksheet.pdf</code></li> </ol>"},{"location":"opensource-projects/asian-character-worksheet-generator/#language-supported","title":"Language Supported","text":"<ol> <li>Chinese</li> <li>Japanese (Hiragana and Katagana)</li> </ol>"},{"location":"opensource-projects/asian-character-worksheet-generator/#sample","title":"Sample","text":"<p>sample worksheet</p>"},{"location":"opensource-projects/asian-character-worksheet-generator/#special-thanks","title":"Special Thanks","text":"<p>My lovely sons Tim and Hin. Also thanks TC Dan for giving valuable feedback to this README file. </p>"},{"location":"opensource-projects/asian-character-worksheet-generator/#github-link","title":"Github link","text":"<p>Asian Character Worksheet Generator</p>"},{"location":"opensource-projects/asian-comprehension-worksheet-generator/","title":"Asian Comprehension Worksheet Generator","text":""},{"location":"opensource-projects/asian-comprehension-worksheet-generator/#background","title":"Background","text":"<p>This is a similar project to Asian Character Worksheet Generator.</p> <p>As an immigrant and father of two boys living in America, I want teach my sons comprehension and reading skills of my language. I want to pick my own material which fits their levels.  </p>"},{"location":"opensource-projects/asian-comprehension-worksheet-generator/#benefit-of-the-project","title":"Benefit of the project","text":"<p>The worksheet comes with empty column next to each word column. You can add phonetic note besides the character or use it to practice writing.</p>"},{"location":"opensource-projects/asian-comprehension-worksheet-generator/#requirements","title":"Requirements","text":"<ol> <li>python3</li> <li>pip</li> </ol> <p>Run the following to install required packages <pre><code>pip install -r requirements.txt\n</code></pre></p>"},{"location":"opensource-projects/asian-comprehension-worksheet-generator/#how-to-use","title":"How to Use","text":"<ol> <li>Edit the file <code>text</code> with the content of your choice (or use the sample). <pre><code>\u5149\u8f1d\u6b72\u6708\n\u9418\u8072\u97ff\u8d77\u6b78\u5bb6\u7684\u4fe1\u865f\uff0c\n\u5728\u4ed6\u751f\u547d\u88e1\uff0c\u5f77\u5f7f\u5e36\u9ede\u550f\u5653\u3002\n\u9ed1\u8272\u808c\u819a\u7d66\u4ed6\u7684\u610f\u7fa9\uff0c\n\u662f\u4e00\u751f\u5949\u737b\uff0c\u819a\u8272\u9b25\u722d\u4e2d\u3002\n\u5e74\u6708\u628a\u64c1\u6709\u8b8a\u505a\u5931\u53bb\uff0c\n\u75b2\u5026\u7684\u96d9\u773c\u5e36\u8457\u671f\u671b\u3002\n\u4eca\u5929\u53ea\u6709\u6b98\u7559\u7684\u8ec0\u6bbc\uff0c\n\u8fce\u63a5\u5149\u8f1d\u6b72\u6708\uff0c\n\u98a8\u96e8\u4e2d\u62b1\u7dca\u81ea\u7531\u3002\n\u4e00\u751f\u7d93\u904e\u508d\u5fa8\u7684\u6399\u624e\uff0c\n\u81ea\u4fe1\u53ef\u6539\u8b8a\u672a\u4f86\uff0c\n\u554f\u8ab0\u53c8\u80fd\u505a\u5230\u3002\n\u53ef\u5426\u4e0d\u5206\u819a\u8272\u7684\u754c\u9650\uff0c\n\u9858\u9019\u571f\u5730\u88e1\uff0c\u4e0d\u5206\u4f60\u6211\u9ad8\u4f4e\u3002\n\u7e7d\u7d1b\u8272\u5f69\u9583\u51fa\u7684\u7f8e\u9e97\uff0c\n\u662f\u56e0\u5b83\u6c92\u6709\uff0c\u5206\u958b\u6bcf\u7a2e\u8272\u5f69\u3002\n</code></pre></li> <li>Generate the worksheet in pdf format with the following command: <pre><code>python3 run.py\n</code></pre></li> <li>Print out the generated file <code>worksheet.pdf</code></li> </ol>"},{"location":"opensource-projects/asian-comprehension-worksheet-generator/#language-supported","title":"Language Supported","text":"<ol> <li>Chinese</li> <li>Japanese (Hiragana and Katagana)</li> </ol>"},{"location":"opensource-projects/asian-comprehension-worksheet-generator/#sample","title":"Sample","text":"<p>sample worksheet</p>"},{"location":"opensource-projects/asian-comprehension-worksheet-generator/#code-overview","title":"Code Overview","text":"<p>Everything is written in python in <code>run.py</code>. You can play with the font and grid size with the variables under the <code># Basic settings</code> section.</p>"},{"location":"opensource-projects/asian-comprehension-worksheet-generator/#contributing","title":"Contributing","text":"<p>I appreciate all suggestions or PRs which will help kids learn Asian language better. Feel free to fork the project and create a pull request with your idea.</p>"},{"location":"opensource-projects/asian-comprehension-worksheet-generator/#special-thanks","title":"Special Thanks","text":"<p>My lovely sons Tim and Hin.</p>"},{"location":"opensource-projects/asian-comprehension-worksheet-generator/#github-link","title":"Github link","text":"<p>Asian Comprehension Worksheet Generator</p>"},{"location":"opensource-projects/job-winner/","title":"Job Winner","text":""},{"location":"opensource-projects/job-winner/#background","title":"Background","text":"<p>I lost my job right before the holiday season in 2022. I need to look for a new job and apply for as many as I can while all the big techs are laying off talents.</p> <p>To keep track of job applications:</p> <ol> <li>Use Google Spreadsheet (I was using it before this project). It is ugly, hard to maintain and easy to mess up the layout.</li> <li>Use online tool like huntr.co. But it can only track 40 jobs and I don't want to spend a dime for the premium service.</li> <li>Use pen and paper. It works but I can't copy and paste information such as link job link and utilize it later.</li> </ol> <p>None of the about is fun or totally fit my use case.</p> <p>That's the reason for me to build something for myself, and potentially you, to get the job done (and to get a new job successfully!)</p>"},{"location":"opensource-projects/job-winner/#benefit-of-job-winner","title":"Benefit of Job Winner","text":"<p>Job Winner helps you keep track of all your job applications in one place\u2014totally free! No hidden fees, and you won't have to worry about your personal data being sold.</p> <p>One of the handy features is the Profile Page. This is where you can store all the personal information that job applications often ask for (like your LinkedIn URL). Plus, it has a super handy feature that lets you copy any field to your clipboard with just a click.</p>"},{"location":"opensource-projects/job-winner/#features","title":"Features","text":"<p>Key Functions:</p> <ol> <li> <p>Index Page:</p> <p>View a list of all your job applications in one place, with easy access to each application\u2019s details and status. </p> </li> <li> <p>Create New Job Application:</p> <p>Easily add new job applications to your list with a user-friendly form. </p> </li> <li> <p>Delete Job Application:</p> <p>Remove any applications you no longer need with a simple delete option.</p> </li> <li> <p>Edit Job Application:</p> <p>Make updates to your existing job applications as the status of your applications change or new information becomes available.  - Manage Interview:</p> <p>Keep track of your interviews with the ability to add and manage interview details.    - Manage Offer:</p> <p>Track any job offers you\u2019ve received, including details about the salary and offer date.   </p> </li> <li> <p>Profile Page:</p> <p>Store and manage all your personal information in one spot </p> </li> <li> <p>Interview List:</p> <p>Keep track of all your interviews in one place, with easy-to-view details and statuses plus sortable headers. </p> </li> <li> <p>Offer List:</p> <p>Track all job offers you\u2019ve received with sortable headers. </p> </li> <li> <p>Search and Filter:</p> <p>Quickly search for specific job applications based on keywords to help you stay organized. Whether it's the company name, job title, or description, finding the right job application is easy.</p> </li> <li> <p>Interview and Offer Count:</p> <p>Stay on top of your job search with an overview of how many interviews and offers you currently have, making it easier to manage multiple opportunities.</p> </li> </ol>"},{"location":"opensource-projects/job-winner/#stack","title":"Stack","text":"<ol> <li>Java 17</li> <li>Spring Boot 3.4.0</li> <li>Spring Reactive</li> <li>GraphQL</li> <li>Postgres DB</li> <li>React</li> <li>Material UI</li> </ol>"},{"location":"opensource-projects/job-winner/#github-links","title":"Github links","text":"<p>Job Winner has two components</p> <ol> <li>Backend</li> <li>UI</li> </ol>"},{"location":"opensource-projects/math-worksheet-generator/","title":"Math Worksheet Generator","text":""},{"location":"opensource-projects/math-worksheet-generator/#background","title":"Background","text":"<p>My best friend tests his 5 year old basic math questions from store-bought material which is good for one time use (his son memorizes the answers) \u2026. but he wants to give him more practice.</p> <p>Two solutions:</p> <ol> <li>keep buying more one time usage materials (less beer budget); or</li> <li>make question sets with the number pairs and calculate the answer for each question manually (less beer time)</li> </ol> <p>Not ideal.</p> <p>That's the reason for me to look into an automate way to get the job done.</p>"},{"location":"opensource-projects/math-worksheet-generator/#benefit-of-the-math-worksheet-generator","title":"Benefit of the Math Worksheet Generator","text":"<p>With the Math Worksheet Generator, you can create a PDF with unique questions, as needed, in a fraction of second.</p> <p>There are five choices:</p> <ol> <li>Addition</li> <li>Subtraction</li> <li>Multiplication</li> <li>Division</li> <li>Mixed</li> </ol>"},{"location":"opensource-projects/math-worksheet-generator/#requirements","title":"Requirements","text":"<p>python3</p> <p>Install required package with the following command: <pre><code>pip install -r requirements.txt\n</code></pre></p>"},{"location":"opensource-projects/math-worksheet-generator/#how-to-use","title":"How to Use","text":"<ol> <li>Generate the worksheet in pdf format with the following command: <pre><code>python3 run.py --type [+|-|x|/|mix] --digits [1|2|3] [-q|--question_count] [int] --output [custom-name.pdf]\n</code></pre></li> <li>Print out the generated file <code>worksheet.pdf</code></li> </ol> <p>For addition only worksheet: <pre><code>python3 run.py --type +\n</code></pre> For calculation up to 3 digits range: <pre><code>python3 run.py --digits 3\n</code></pre> For generating different number of question, eg. 100 (default is 80): <pre><code>python3 run.py -q 100\n</code></pre> or <pre><code>python3 run.py --question_count 100\n</code></pre> For custom output filename (default is worksheet.pdf): <pre><code>python3 run.py --output custom-name.pdf\n</code></pre></p>"},{"location":"opensource-projects/math-worksheet-generator/#sample","title":"Sample","text":"<p>sample worksheet</p>"},{"location":"opensource-projects/math-worksheet-generator/#special-thanks","title":"Special Thanks","text":"<p>My long time friend San for the inspiration of this project and lovely sons Tim and Hin. Thanks thedanimal for reviewing this README and adding new features.</p> <p>Also, thank you for the love and support form the Reddit Python community. You guys are amazing and are helping me to make this project better.</p>"},{"location":"opensource-projects/math-worksheet-generator/#successful-story","title":"Successful Story","text":"<p>Thanks k1m0ch1 for sharing this heartwarming story:</p> <p>...I made this card for my kid, and then the teacher asks me if I can make some for the kids, well its generated anyway and very helpful, and the next day he asks me to make for a whole class, and next day he wants me to make for a whole school, and a weeks later other schools want me to help to make for a whole school.       more than 1000 generated file, with a custom filename for every kid and sent to the email     I'm doing it for free, while you made this free, love it &lt;3</p>"},{"location":"opensource-projects/math-worksheet-generator/#coverage","title":"Coverage","text":"<p>The project was featured in the following links:</p> <p>PyCoder's Weekly Issue #442</p> <p>PyCoder's Weekly Twitter</p> <p>Real Python Facebook</p> <p>Github Trends Telegram</p> <p>Python Trending Twitter</p>"},{"location":"opensource-projects/math-worksheet-generator/#github-link","title":"Github link","text":"<p>Math Worksheet Generator</p>"},{"location":"tech-blog/devops/create-erc-public-login/","title":"Create erc public login","text":"<p>Amazon ECR Public allows users to store and access public container images. While ECR Public repositories are open to the public, access to pull or download images from these repositories may still require authentication.</p> <p>While there are multiple reasons such as access control and security concern, the main benefit of getting an authentication token or login is to deal with rate limiting in my use case.</p>"},{"location":"tech-blog/devops/create-erc-public-login/#get-a-ecr-public-token","title":"Get a ECR-Public Token","text":"<p>In so doing, you can create a kubernetes secret from the token to be accessible within the namespace:</p> <pre><code>TOKEN=$(aws ecr-pubic get-authorization-token --region us-east-1)\nNS=\"THE_NAMESPACE\"\n\nkubectl create secret docker-registry ecr-public-secret \\\n  --namespace $NS \\\n  --docker-username=AWS\n  --docker-password=\"$TOKEN\" \\\n  --docker-server=\"public.erc.aws\"\n  --dry-run=client -o yaml | kubectl apply -f -\n</code></pre>"},{"location":"tech-blog/devops/create-erc-public-login/#get-a-ecr-public-login","title":"Get a ECR-Public Login","text":"<p>Another option is to get the login password and then login to the ECR public registry. This option is more suitable for CI/CD automation script.</p> <pre><code>aws ecr-public get-login-password --region us-east-1 | docker login --username AWS --password-stdin public.ecr.aws\n</code></pre>"},{"location":"tech-blog/devops/create-erc-public-login/#refresh-tokenlogin","title":"Refresh Token/Login","text":"<p>The token/login has a validity period of 12 hours. You need to obtain a new one to continue accessing ECR Public.</p> <p>The follow cron job will refresh the Kubernetes secret every 6 hours</p> <pre><code>apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: refresh-ecr-public-token\nspec:\n  schedule: \"0 */6 * * *\"  # Runs every 6 hours\n  jobTemplate:\n    spec:\n      template:\n        spec:\n          containers:\n          - name: refresh-token\n            image: awscli/awscli:latest  # Use an image with AWS CLI installed\n            command: [\"sh\", \"-c\"]\n            args:\n            - |\n              # Refresh ECR Public token\n              token=$(aws ecr-public get-login-password --region us-east-1)\n              kubectl create secret docker-registry ecr-public-secret \\\n                --docker-server=public.ecr.aws \\\n                --docker-username=AWS \\\n                --docker-password=$token\n          restartPolicy: OnFailure\n</code></pre>"},{"location":"tech-blog/devops/dev-container/","title":"Dev Container","text":""},{"location":"tech-blog/devops/dev-container/#background","title":"Background","text":"<p>Since my first job after college graduation, I have been looking for a way to setup a portable development environment for the following reasons:</p> <ol> <li>to use it on multiple machines</li> <li>to share it among the team with teammates who doesn't have server/platform/os experience</li> <li>to standardize os, programming language and library version, etc</li> <li>to save myself the headache of rebuilding everything from scratch</li> </ol> <p>All throught these years, I have been using the following to achieve this:</p> <ol> <li>Vmware/Virtualbox</li> <li>Puppet</li> <li>Docker</li> </ol> <p>I thought the endgame would be docker since it is lightweight, scriptable and can be run almost anywhere, until I meet Dev Container.</p>"},{"location":"tech-blog/devops/dev-container/#about-dev-container","title":"About Dev Container","text":"<p>Dev Container is a feature of VS Code and Docker, which provides an easy and reproducible way to set up development environments, within a button residing on the IDE. It can be used to run an application, to separate tools, libraries, or runtimes needed for working with a codebase</p>"},{"location":"tech-blog/devops/dev-container/#prerequisite","title":"Prerequisite","text":"<ol> <li>VS Code</li> <li>Docker</li> </ol>"},{"location":"tech-blog/devops/dev-container/#to-create-a-dev-container","title":"To Create a Dev Container","text":"<p>As an example, I am going to create a Python 3.9 Dev Container with Docker in Docker (DIID) support</p> <ol> <li>Click on the lower left green icon at the VS Code IDE</li> <li>From the dropdown at the top portion of the VS Code, select <code>Create Dev Container</code> </li> <li>Search for Python 3 and select <code>Python 3 devcontainers</code> </li> <li>Select <code>Additional Options</code> and choose Python 3.9 </li> <li>Search for <code>docker-in-docker</code> and check the checkbox </li> <li>Click OK</li> </ol> <p>At this point, VS Code will pop up an information box about the creation of the new Dev Container. After that VS Code will restart in the Dev Container mode.</p> <p></p> <p>There you have it. You should be able to run your application inside the Dev Container with Python 3.9 now.</p> <p></p>"},{"location":"tech-blog/devops/dev-container/#configuration-file","title":"Configuration file","text":"<p>The configuration of the new Dev Container is saved under <code>.devcontainer/devcontainer.json</code> file. Here is the content of the file.</p> <pre><code>// For format details, see https://aka.ms/devcontainer.json. For config options, see the\n// README at: https://github.com/devcontainers/templates/tree/main/src/python\n{\n    \"name\": \"Python 3\",\n    // Or use a Dockerfile or Docker Compose file. More info: https://containers.dev/guide/dockerfile\n    \"image\": \"mcr.microsoft.com/devcontainers/python:1-3.9-bookworm\",\n    \"features\": {\n        \"ghcr.io/devcontainers/features/docker-in-docker:2\": {}\n    }\n\n    // Features to add to the dev container. More info: https://containers.dev/features.\n    // \"features\": {},\n\n    // Use 'forwardPorts' to make a list of ports inside the container available locally.\n    // \"forwardPorts\": [],\n\n    // Use 'postCreateCommand' to run commands after the container is created.\n    // \"postCreateCommand\": \"pip3 install --user -r requirements.txt\",\n\n    // Configure tool-specific properties.\n    // \"customizations\": {},\n\n    // Uncomment to connect as root instead. More info: https://aka.ms/dev-containers-non-root.\n    // \"remoteUser\": \"root\"\n}\n</code></pre>"},{"location":"tech-blog/devops/dev-container/#link","title":"Link","text":"<p>Dev Container</p> <p>Github Repo</p>"},{"location":"tech-blog/devops/dynamodb/","title":"Dynamodb","text":""},{"location":"tech-blog/devops/dynamodb/#list-all-tables","title":"List all tables","text":"<pre><code>aws dynamodb scan list-tables\n</code></pre>"},{"location":"tech-blog/devops/dynamodb/#backup-a-table","title":"Backup a table","text":"<pre><code>aws dynamodb scan --table-name TABLE-NAME &gt; backup.json\n</code></pre>"},{"location":"tech-blog/devops/dynamodb/#get-a-single-record-by-id","title":"Get a single record by ID","text":"<pre><code>aws dynamodb get-item --table-name TABLE-NAME --key '{\"id\": {\"S\":\"the-id-to-look-for\"}}'\n</code></pre>"},{"location":"tech-blog/devops/dynamodb/#delete-a-single-record-by-id","title":"Delete a single record by ID","text":"<pre><code>aws dynamodb delete-item --table-name TABLE-NAME --key '{\"id\": {\"S\":\"the-id-to-look-for\"}}'\n</code></pre>"},{"location":"tech-blog/devops/github-multi-repo-update/","title":"Github multi repo update","text":""},{"location":"tech-blog/devops/github-multi-repo-update/#originally-posted-at-linkedin-at-july-15-2023","title":"originally posted at LinkedIn at July 15, 2023","text":""},{"location":"tech-blog/devops/github-multi-repo-update/#background","title":"Background","text":"<p>This is related to the other blog post about GitHub Shared Action.</p> <p>I want to make the same change to multiple GitHub repos and I want to limit manual steps as much as possible. As an example, I would like to push the the files of a source folder (source) to three of my repos. Here is one of the file:</p> <p><code>source/somefile</code> <pre><code>Hello World!\n</code></pre></p> <p>The following file contains the repo name that I want to update:</p> <p><code>repos.txt</code></p> <pre><code>mock-flask\njob-winner\nrock-paper-scissors\n</code></pre>"},{"location":"tech-blog/devops/github-multi-repo-update/#original-plan","title":"Original Plan","text":"<p>My first thought was to implement a single bash script to do all the work:</p> <ol> <li>read the repos.txt file line by line</li> <li>for each line, run all the require git commands one by one</li> </ol> <p>Here is the snippet for the original plan:</p> <pre><code>REPO=$1\nREPO_ROOT=\"/Users/janus/workspace/\"\nSOURCE_DIR=\"source\"\n\nwhile read -r line; do\n    cd \"$line\"\n    cp $SOURCE_DIR/* $REPO_ROOT/$REPO/\n    cd $REPO_ROOT/$REPO/\n    git checkout -b TEST-0000\n    git add .\n    git commit -m \"TEST-0000 patching\"\n    git push origin TEST-0000\ndone &lt; repos.txt\n</code></pre> <p>This can get the job done, but what if I have to do similar update in the future?</p>"},{"location":"tech-blog/devops/github-multi-repo-update/#build-a-reusable-script","title":"Build a Reusable Script","text":"<p>As a DevOps engineerer, I expect a similar task to patch multiple repos will come over and over again. Therefore, I break down the script into two parts:</p> <ol> <li>The reusable part - one that read the repos.txt file and perform the while loop<ul> <li>no need to reinvent the wheel anymore</li> <li>no need to test if the looping the repos feature in the future</li> </ul> </li> <li>The pluggable part - the script that does the GitHub update<ul> <li>can be versionized</li> <li>can be tested easier as it can be tested against a single repo as a standalone script</li> </ul> </li> </ol> <p>Here are the new implementation:</p> <p><code>github-looping.sh</code> <pre><code>#!/bin/sh\n\nMY_SCRIPT=$1\n\nwhile read -r line; \n    do ./$MY_SCRIPT \"$line\"; \ndone &lt; repos.txt\n</code></pre></p> <p><code>TEST-0000.sh</code> - while TEST-0000 is the jira ticket number <pre><code>#!/bin/sh\n\nGH_BRANCH=$(basename -- $0 | cut -d. -f1)\nREPO=$1\nSOURCE_DIR=\"source\"\nREPO_ROOT=\"/Users/janus/workspace/\"\nCOMMIT_MESSAGE=\"mass patching\"\n\necho \"processing repo \"$REPO\" ...\"\ncp $SOURCE_DIR/* $REPO_ROOT/$REPO/\ncd $REPO_ROOT/$REPO/\ngit checkout -b $GH_BRANCH\ngit add .\ngit commit -m \"$GH_BRANCH $COMMIT_MESSAGE\"\ngit push origin $GH_BRANCH\necho \"finish processing repo \"$REPO\" ...\"\n</code></pre></p> <p>To run the update across all the repos, simply run the following: <code>./github-looping.sh TEST-0000.sh</code></p> <p>The idea is to have a seperate script being called by <code>github-looping.sh</code>. </p> <p>In so doing, I can versionize the TEST-XXXX.sh script and have an idea of what exactly was being run.</p>"},{"location":"tech-blog/devops/github-multi-repo-update/#conclusion","title":"Conclusion","text":"<p>There are probably a thousand better ways out there to do similar thing. I found couple packages out there that take user inputs from the command lines interactively. Some of them would print out a nice report too. There is certainly room for improvement. However, as long as I can save myself from the headache of manaully update indivdual repos, I am happy with what I have so far.</p> <p>Hope this works for you too if you are facing a similar problem.</p>"},{"location":"tech-blog/devops/github-repository-dispatch/","title":"Github repository dispatch","text":""},{"location":"tech-blog/devops/github-repository-dispatch/#background","title":"Background","text":"<p>I have to run a shell script at repo B from repo A which hosts a python app. While I can spend the time to reinvent the wheel and fork the logic into python, it is best to keep thing simple by reusing what is already out there and proved to be working fine.</p> <p>The js script in repo B is already wired up with a Github workflow. What if I can kick off that workflow remotely from repo A?</p> <p>Github repository dispatch is the answer.</p>"},{"location":"tech-blog/devops/github-repository-dispatch/#about-github-repository-dispatch","title":"About Github Repository Dispatch","text":"<p>Github repository dispatch is a feature to trigger custom events or actions programmatically. It allows you to create a webhook endpoint that can receive external HTTP requests and generate a repository-level event. This event can then be used to trigger workflows, CI/CD pipelines, or any other custom actions defined in your repository.</p>"},{"location":"tech-blog/devops/github-repository-dispatch/#sample-workflow","title":"Sample Workflow","text":"<p>This sample workflow will print out the client payload message.</p> <pre><code>name: dispatch receiver test\n\non: \n  repository_dispatch:\n    types: [my-type]\n\njobs:\n  build:\n    runs-on: ubuntu-latest\n    steps:\n    - uses: actions/checkout@v1\n    - name: dispatch trigger\n      env:\n        MESSAGE: ${{ github.event.client_payload.message }}\n      run: |\n        echo \"$MESSAGE\"\n</code></pre>"},{"location":"tech-blog/devops/github-repository-dispatch/#github-personal-token","title":"Github Personal Token","text":"<p>You will need to create a personal token for the repo, in this case, repo B, with the following setting:</p> <p><code>permissions contents: write</code></p> <p>Find more about how to create a token.</p>"},{"location":"tech-blog/devops/github-repository-dispatch/#sample-code-to-trigger-the-event","title":"Sample Code To trigger the event","text":"<p>Curl</p> <pre><code>curl -H \"Accept: application/vnd.github.everest-preview+json\" \\\n    -H \"Authorization: token $TOKEN\" \\\n    --request POST \\\n    --data '{\"event_type\": \"my-type\", \"client_payload\": {\"message\": \"Testing Github repository dispatch\"}}' \\\n    https://api.github.com/repos/&lt;org&gt;/&lt;repo&gt;/dispatches\n</code></pre> <p>Python <pre><code>import json\nimport requests\nfrom loguru import logger\n\nORG = \"my-org\"\nREPO = \"my-repo\"\nTOKEN = \"my-secret-token\"\n\ndef get_github_dispatch_response(team_name):\n    external_repo = f\"https://api.github.com/repos/{ORG}/{REPO}/dispatches\"\n    data = {\"event_type\": \"my-type\"}\n    message = {\"message\": f\"Triggered by workflow dispatch\"}\n    payload = {\"client_payload\": message}\n    data.update(payload)\n    headers = {\"Authorization\": f\"Bearer {TOKEN}\"}\n    response = requests.post(external_repo, data=json.dumps(data), headers=headers)\n    if response.status_code == 204:\n        return response\n    logger.error(f\"Workflow dispatch failed. Error code and message: {response.status_code} - {response.text}\")\n    response.raise_for_status()\n</code></pre></p>"},{"location":"tech-blog/devops/github-repository-dispatch/#link","title":"Link","text":"<p>Repository Dispatch</p> <p>How to create a token</p>"},{"location":"tech-blog/devops/github-shared-action/","title":"Github shared action","text":""},{"location":"tech-blog/devops/github-shared-action/#originally-posted-at-linkedin-at-july-7-2023","title":"originally posted at LinkedIn at July 7, 2023","text":""},{"location":"tech-blog/devops/github-shared-action/#background","title":"Background","text":"<p>I need to set up 3 GitHub workflows for 10 repo. For each workflow, it will run a similar bash script.</p>"},{"location":"tech-blog/devops/github-shared-action/#first-trial-create-separate-bash-script-and-workflow","title":"First trial - Create Separate Bash Script and Workflow","text":"<p>On my first trial, I did the following:</p> <ol> <li>work on a single repo</li> <li>create three separate bash scripts</li> <li>create three separate workflows</li> </ol> <p>Here are the three bash scripts:</p> <p>run-dependabot-alert.sh <pre><code>#!/bin/sh\n\nTOKEN=$1\nREPO=$2\n\nget_response(){\n  curl -sL \\\n    -H \"Accept: application/vnd.github+json\" \\\n    -H \"Authorization: Bearer $TOKEN\"\\\n    -H \"X-GitHub-Api-Version: 2022-11-28\" \\\n    https://api.github.com/repos/januschung/\"$REPO\"/dependabot/alerts\n}\n\ndo_something(){\n    echo \"Running Dependabot!\"\n}\n\nget_response\ndo_something\n</code></pre> run-secret-scanning.sh <pre><code>#!/bin/sh\n\nTOKEN=$1\nREPO=$2\n\nget_response(){\n  curl -sL \\\n    -H \"Accept: application/vnd.github+json\" \\\n    -H \"Authorization: Bearer $TOKEN\"\\\n    -H \"X-GitHub-Api-Version: 2022-11-28\" \\\n    https://api.github.com/repos/januschung/\"$REPO\"/secret-scanning/alerts\n}\n\ndo_something(){\n    echo \"Running Secret Scanning!\"\n}\n\nget_response\ndo_something\n</code></pre></p> <p>run-codeql-alert.sh <pre><code>#!/bin/sh\n\nTOKEN=$1\nREPO=$2\n\nget_response(){\n  curl -sL \\\n    -H \"Accept: application/vnd.github+json\" \\\n    -H \"Authorization: Bearer $TOKEN\"\\\n    -H \"X-GitHub-Api-Version: 2022-11-28\" \\\n    https://api.github.com/repos/januschung/\"$REPO\"/code-scanning/alerts\n}\n\ndo_something(){\n    echo \"Running CodeQL Scanning!\"\n}\n\nget_response\ndo_something\n</code></pre> Here is one of the three workflow, as they are more like the same:</p> <p>dependabot-workflow.yml <pre><code>name: 'Dependabot Alert Check'\non:\n  push:\n    branches:\n      - 'main'\n\njobs:\n  dependabot-scanning-alert-check:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n        with:\n          token: ${{ secrets.GH_TOKEN }}\n      - name: Run Dependabot Alert Check\n        shell: bash\n        run: |\n          ./scripts/run-dependabot-alert.sh ${{ secret.GH_TOKEN }} ${{ github.event.repository.name }}\n</code></pre></p> <p>As you can see, there are a lot of similar logic from both of the bash scripts and the workflows. It will be tedious to repeat the work across the other repos.</p> <p>Can I do better? Sure!</p>"},{"location":"tech-blog/devops/github-shared-action/#second-trial-create-shareable-composite-action","title":"Second Trial - Create Shareable Composite Action","text":"<p>Since all of the 10 repo will share the same logic, I converted the workflow into a shareable composite action in a dedicated repo. In so doing, the other repo can reference the same action. Here are the benefits:</p> <ul> <li>a single source of truth</li> <li>easier to maintain one set of bash scripts and action</li> <li>easier to get the rest of the repo onboard</li> </ul> <p>Here is the shareable composite action:</p> <pre><code>name: 'Alert Check'\ndescription: 'Run alert check'\ninputs:\n  alert-type:\n    description: 'Alert type to run - code-scanning, dependabot or secret-scanning'\n    required: true\n    default: 'dependabot'\n  gh-token:\n    required: true\n\nruns:\n  using: \"composite\"\n  steps:\n    - uses: actions/checkout@v3\n      with:\n        token: ${{ inputs.gh-token }}\n    - shell: bash\n      run: echo \"${{ github.action_path }}\" &gt;&gt; $GITHUB_PATH\n    - shell: bash\n      run: |\n        ${{ alert-type }}.sh ${{ inputs.gh-token }} ${{ github.event.repository.name }}\n</code></pre> <p>Note that the bash scripts are now migrated from <code>scripts</code> folder into <code>.github/actions/alerts</code></p> <p>The corresponding workflow that reference the shareable composite action:</p> <pre><code>on: [push]\n\njobs:\n  run_alert:\n    runs-on: ubuntu-latest\n    name: Run Dependabot Alert\n    steps:\n      - uses: actions/checkout@v3\n      - uses: januschung/shared-action-repo/.github/actions/alerts@main\n        with:\n          gh-token: ${{ secrets.GH_TOKEN }}\n          alert-type: ${{ matrix.type }}\n</code></pre> <p>Looking way much better now. But, can I do even better?</p>"},{"location":"tech-blog/devops/github-shared-action/#third-trial-consolidate-into-a-single-bash-script-and-a-single-action","title":"Third Trial - Consolidate Into a Single Bash Script and a Single Action","text":"<p>Since all three bash scripts share 80% of the same logic, I consolidated them into a single one:</p> <p>alert.sh <pre><code>#!/bin/sh\n\nTOKEN=$1\nREPO=$2\nTYPE=$3\n\nget_response(){\n  curl -sL \\\n    -H \"Accept: application/vnd.github+json\" \\\n    -H \"Authorization: Bearer $TOKEN\"\\\n    -H \"X-GitHub-Api-Version: 2022-11-28\" \\\n    https://api.github.com/repos/januschung/\"$REPO\"/\"$TYPE\"/alerts\n}\n\ndo_something(){\n    if [ \"TYPE\" == 'code-scanning' ]; then\n        do_something_code_scanning\n    elif [ \"TYPE\" == 'dependabot' ]; then\n        do_something_dependabot\n    elif [ \"TYPE\" == 'secret-scanning' ]; then\n        do_something_secret_scanning\n    else\n        exit 1\n    fi\n}\n\ndo_something_code_scanning(){\n    echo \"Running Secret Scanning!\"\n}\n\ndo_something_dependabot(){\n    echo \"Running Dependabot!\"\n}\n\ndo_something_secret_scanning(){\n    echo \"Running Secret Scanning!\"\n}\n\nget_response\ndo_something\n</code></pre></p> <p>The consolidated action.yml</p> <pre><code>name: 'Alert Check'\ndescription: 'Run alert check'\ninputs:\n  alert-type:\n    description: 'Alert type to run - code-scanning, dependabot or secret-scanning'\n    required: true\n    default: 'dependabot'\n  gh-token:\n    required: true\n\nruns:\n  using: \"composite\"\n  steps:\n    - uses: actions/checkout@v3\n      with:\n        token: ${{ inputs.gh-token }}\n    - shell: bash\n      run: echo \"${{ github.action_path }}\" &gt;&gt; $GITHUB_PATH\n    - shell: bash\n      run: |\n        alert.sh ${{ inputs.gh-token }} ${{ github.event.repository.name }} {{ inputs.alert-type }}\n</code></pre> <p>I do not want to create three separate workflows from the external repo either, so I use GitHub Matrix in a single workflow:</p> <pre><code>on: [push]\n\njobs:\n  run_alert:\n    runs-on: ubuntu-latest\n    name: Run Alert\n    strategy:\n      matrix:\n        type: ['code-scanning', 'dependabot', 'secret-scanning']\n    steps:\n      - uses: actions/checkout@v3\n      - uses: januschung/shared-action-repo/.github/actions/alerts@main\n        with:\n          gh-token: ${{ secrets.GH_TOKEN }}\n          alert-type: ${{ matrix.type }}\n</code></pre> <p>That is it! I hope this helps if you are into similar situation in the coming future.</p>"},{"location":"tech-blog/devops/github-shared-action/#links","title":"Links","text":"<p>Composite Action</p> <p>Matrix</p>"},{"location":"tech-blog/devops/kubernetes-reference/","title":"Kubernetes reference","text":""},{"location":"tech-blog/devops/kubernetes-reference/#basic","title":"Basic","text":"<pre><code># export KUBECONFIG\nexport KUBECONFIG=~/.kube/[config-file]\n\n# create a namespace\nk create ns [namespace]\n\n# check current cluster\nk config current-context\nk config get-contexts\n\n# describe\nk describe [type] [name] -n [namespace]\n\n# get everything\nk get all\nk get [type] -A\n\n# drain a node https://kubernetes.io/docs/tasks/administer-cluster/safely-drain-node/\nk drain [node] --ignore-daemonsets --delete-emptydir-data\n\n# edit a deployment\nk edit deployment [deployment] -n [namespace]\n\n# set 0 pod for a deployment\nk scale deployment [deployment] -n [namespace] --replicas=0\n</code></pre>"},{"location":"tech-blog/devops/kubernetes-reference/#dbinstance","title":"DBInstance","text":"<pre><code>k get dbinstance [instance-name] -n [namespace]\n\n# to get address and port\nk get dbinstance [instance-name] -n [namespace] -o json | jq -r '.status.endpoint.address'\nk get dbinstance [instance-name] -n [namespace] -o json | jq -r '.status.endpoint.port'\n</code></pre>"},{"location":"tech-blog/devops/kubernetes-reference/#helm","title":"Helm","text":"<pre><code># sample workflow to install prometheus from helm\nhelm repo add prometheus-community https://prometheus-community.github.io/helm-charts\nhelm search repo prometheus-community\nhelm upgrade --install -n istio-system prometheus prometheus-community/prometheus\n\n# to install with customized value file and specific version\nhelm upgrade --install -n istio-system prometheus prometheus-community/prometheus \\\n  -f values.yaml \\\n  --version 23.3.0\n\n# to uninstall\nhelm uninstall prometheus\n\n# to check template to be generated\nhelm template . -f values.yaml -f values2.yaml\n</code></pre>"},{"location":"tech-blog/devops/kubernetes-reference/#initcontainer","title":"InitContainer","text":"<p>Init containers are speicalized containers that run before app containers in the same pod. One usage of init container is to download file and share it with the app container via shared volume. To learn more, check out official Kubernetes documentation.</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: myapp\n  labels:\n    app: myapp\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: mydb\n  template:\n    metadata:\n      labels:\n        app: mydb\n    spec:\n      containers:\n        - name: mydb\n          image: postgres\n          env:\n          - name: POSTGRES_PASSWORD\n            value: example\n          ports:\n          - containerPort: 5432\n            name: postgres\n          volumeMounts:\n            - name: data\n              mountPath: /docker-entrypoint-initdb.d\n      initContainers:\n        - name: curl-downloader\n          image: appropriate/curl\n          args:\n            - \"-o\"\n            - \"/tmp/data/init.sql\"\n            - \"https://raw.githubusercontent.com/januschung/support-system-db/main/sql-scripts/create_tables.sql\"\n          volumeMounts:\n            - name: data\n              mountPath: /tmp/data\n      volumes:\n        - name: data\n          emptyDir: {}\n</code></pre>"},{"location":"tech-blog/devops/react-ruby-docker-dev-environment/","title":"React and Ruby Docker Development Environment","text":""},{"location":"tech-blog/devops/react-ruby-docker-dev-environment/#background","title":"Background","text":"<p>Lately I am working on a project to dockerize an e2e application with React as the front end and Ruby on Rails as the backend. I would like to set up a docker-compose file so that:</p> <ol> <li>user can bring up the e2e environment with a single command</li> <li>the environment supports hot reload of code for both the React and Ruby on Rails parts</li> <li>it can be served as a testing environment for e2e test</li> <li>developers can run manual test in a local environment</li> </ol>"},{"location":"tech-blog/devops/react-ruby-docker-dev-environment/#react-dockerfile","title":"React Dockerfile","text":"<pre><code>FROM node:lts-alpine3.17\n\nWORKDIR /app\n\nCOPY package.json /app/\nCOPY yarn.lock /app/\nRUN yarn install\n\nCOPY . /app\n\nRUN yarn run build\nCMD yarn start\n</code></pre>"},{"location":"tech-blog/devops/react-ruby-docker-dev-environment/#docker-compose","title":"docker-compose","text":"<pre><code>version: '3'\nservices:\n  api:\n    build: ./backend\n    command: bundle exec rails server -p 3000 -b '0.0.0.0'\n    ports:\n      - \"3000:3000\"\n    volumes:\n      - \"./backend/:/rails\"\n  web:\n    build: ./frontend\n    environment:\n      REACT_APP_BACKEND_URL: \"http://localhost:3000\"\n      WATCHPACK_POLLING: \"true\"\n    command: npm start\n    ports:\n      - \"8080:3000\"\n    volumes:\n      - ./frontend/:/app\n    depends_on:\n      - api\n</code></pre> <p>note that the followings are essential to enable hot reload for both React and Rails:</p>"},{"location":"tech-blog/devops/react-ruby-docker-dev-environment/#react-hot-reload","title":"React Hot Reload","text":"<pre><code>  web:\n    ...\n    environment:\n      WATCHPACK_POLLING: \"true\"\n    command: npm start\n    ...\n    volumes:\n      - ./frontend/:/app\n</code></pre>"},{"location":"tech-blog/devops/react-ruby-docker-dev-environment/#ruby-on-rails-hot-reload","title":"Ruby on Rails Hot Reload","text":"<pre><code>  api:\n    ...\n    command: bundle exec rails server -p 3000 -b '0.0.0.0'\n    ...\n    volumes:\n      - \"./backend/:/rails\"\n</code></pre>"},{"location":"tech-blog/devops/react-ruby-docker-dev-environment/#connecting-front-end-to-back-end","title":"Connecting Front End to Back End","text":"<p>When the React application was trying to consume the Rails API, CORS error was reported. The following is the fix to make them communicate properly:</p>"},{"location":"tech-blog/devops/react-ruby-docker-dev-environment/#backendgemfile","title":"backend/Gemfile","text":"<pre><code>gem \"rack-cors\"\n</code></pre>"},{"location":"tech-blog/devops/react-ruby-docker-dev-environment/#backendconfiginitializerscorsrb","title":"backend/config/initializers/cors.rb","text":"<pre><code>Rails.application.config.middleware.insert_before 0, Rack::Cors do\n  allow do\n    origins 'localhost:3000',\n            '127.0.0.1:3000',\n            'localhost:8080',\n            '127.0.0.1:8080'\n\n    resource '*',\n             headers: :any,\n             methods: %i[get post put patch delete options head]\n  end\nend\nRails.application.config.hosts += ['localhost:3000',\n                                   'localhost',\n                                   '127.0.0.1:3000',\n                                   'localhost:8080',\n                                   '127.0.0.1:8080'\n                                ]\n</code></pre>"},{"location":"tech-blog/devops/react-ruby-docker-dev-environment/#sample-react-component-to-consume-ruby-on-rails-api","title":"Sample React Component to Consume Ruby on Rails API","text":"<pre><code>import React, { useState, useEffect }  from 'react';\nconst BackendTest = () =&gt; {\nconst [error, setError] = useState(null);\n    const [isLoaded, setIsLoaded] = useState(false);\n    const [foo, setFoo] = useState([]);\n    useEffect(() =&gt; {\n        fetch(process.env.REACT_APP_BACKEND_URL + \"/v0/health-check\")\n            .then(res =&gt; res.json())\n            .then(\n                (data) =&gt; {\n                    setIsLoaded(true);\n                    setFoo(data);\n                },\n                (error) =&gt; {\n                    setIsLoaded(true);\n                    setError(error);\n                }\n            )\n      }, [])\nif (error) {\n        return &lt;div&gt;Error: {error.message}&lt;/div&gt;;\n    } else if (!isLoaded) {\n        return &lt;div&gt;Loading...&lt;/div&gt;;\n    } else {\n        console.log(foo)\n        return (\n            &lt;div&gt;{foo.data}&lt;/div&gt;\n        );\n    }\n}\nexport default BackendTest;\n</code></pre>"},{"location":"tech-blog/devops/redis-jedis-setup/","title":"Using Redis with SpringBoot","text":"<p>On my other project, I use Redis as a caching layer. This time I would like to use it as a database in a SprintBoot project.</p>"},{"location":"tech-blog/devops/redis-jedis-setup/#bring-up-a-local-redis-with-docker","title":"Bring up a local Redis with docker","text":"<pre><code>docker run --name my-redis -p 6379:6379 -d redis\n</code></pre>"},{"location":"tech-blog/devops/redis-jedis-setup/#redis-client","title":"Redis Client","text":"<p>There are two popular clients - <code>Lettuce</code> and <code>Jedis</code>. I picked Jedis this time.</p> <pre><code>&lt;!-- pom.xml --&gt;\n&lt;dependency&gt;\n  &lt;groupId&gt;redis.clients&lt;/groupId&gt;\n  &lt;artifactId&gt;jedis&lt;/artifactId&gt;\n&lt;/dependency&gt;\n</code></pre> <p>Local development environment setup:</p> <pre><code># application.properties\n\nspring.redis.host=localhost\nspring.redis.port=6379\n</code></pre> <p>Sample configuration class</p> <pre><code>// configs/RedisConfig.java\n\nimport org.springframework.context.annotation.Bean;\nimport org.springframework.context.annotation.Configuration;\nimport org.springframework.data.redis.connection.RedisConnectionFactory;\nimport org.springframework.data.redis.connection.jedis.JedisConnectionFactory;\nimport org.springframework.data.redis.core.RedisTemplate;\nimport org.springframework.data.redis.serializer.GenericToStringSerializer;\n\n@Configuration\npublic class RedisConfig {\n\n    @Bean\n    public RedisConnectionFactory redisConnectionFactory() {\n        return new JedisConnectionFactory();\n    }\n\n    @Bean\n    public RedisTemplate&lt;String, Object&gt; redisTemplate() {\n        RedisTemplate&lt;String, Object&gt; template = new RedisTemplate&lt;&gt;();\n        template.setConnectionFactory(redisConnectionFactory());\n        template.setValueSerializer(new GenericToStringSerializer&lt;&gt;(Object.class));\n        return template;\n    }\n}\n</code></pre> <p>Sample model class</p> <pre><code>package com.tnite.redistest;\n\nimport lombok.Data;\nimport org.springframework.data.annotation.Id;\nimport org.springframework.data.redis.core.RedisHash;\n\nimport java.io.Serializable;\n\n@Data\n@RedisHash(\"Employee\")\npublic class Employee implements Serializable {\n\n    @Id\n    private Long id;\n    private String name;\n    private Double salary;\n    ...\n}\n</code></pre> <p>Everything was going smoothly until I realized that the ElastiCache instance is SSL-enabled and password-protected.</p>"},{"location":"tech-blog/devops/redis-jedis-setup/#make-it-works-with-elasticache","title":"Make it works with Elasticache","text":"<p>I want to retrieve all configuration parameters from environment variables. To ensure compatibility with the local development setup, the following code can handle configurations without a password and with SSL disabled.</p> <pre><code>@Slf4j\n@Configuration\npublic class RedisConfig {\n\n    @Value(\"${spring.redis.host:localhost}\")\n    private String host;\n\n    @Value(\"${spring.redis.port:6379}\")\n    private Integer port;\n\n    @Value(\"${spring.redis.password:@null}\")\n    private String password;\n\n    @Value(\"${spring.redis.ssl.enabled:false}\")\n    private Boolean useSsl;\n\n    @Bean\n    RedisConnectionFactory redisConnectionFactory() {\n\n        JedisConnectionFactory factory;\n        JedisClientConfiguration config = JedisClientConfiguration.builder().build();\n        if (useSsl) {\n            log.info(\"Redis with SSL enabled!\");\n            config = JedisClientConfiguration.builder().useSsl().build();\n        }\n\n        RedisStandaloneConfiguration redisConfig = new RedisStandaloneConfiguration(host, port);\n\n        if (password != null &amp;&amp; !password.isEmpty()) {\n            log.info(\"Redis with password is used!\");\n            redisConfig.setPassword(password);\n        }\n\n        factory = new JedisConnectionFactory(redisConfig, config);\n\n        return factory;\n    }\n\n}\n</code></pre>"},{"location":"tech-blog/devops/ruby-note/","title":"Ruby notes","text":"<p>use rbenv to manage ruby version</p> <pre><code>brew install rbenv ruby-build\nrbenv init\n\n# list latest stable versions:\nrbenv install -l\n\n# list all local versions:\nrbenv install -L\n\n# install a Ruby version:\nrbenv install 3.1.2\n\nrbenv global 3.1.2   # set the default Ruby version for this machine\n# or:\nrbenv local 3.1.2    # set the Ruby version for this directory\n\n# set the following in .zshrc\neval \"$(rbenv init - zsh)\"\n</code></pre>"},{"location":"tech-blog/devops/ruby-note/#add-support-of-postgres","title":"Add support of postgres","text":"<p>To use Postres DB, gem \"pg\" is required. </p> <p>Run the following if <code>bundle install</code> fails to work. <pre><code>brew install postgresql\n</code></pre></p>"},{"location":"tech-blog/devops/ruby-note/#seed-db-to-create-a-user","title":"Seed db to create a user","text":"<p>We can seed the database with plain sql  <pre><code>connection = ActiveRecord::Base.connection()\nconnection.execute(\"SQL GOES HERE\")\n</code></pre></p>"},{"location":"tech-blog/devops/ssl/","title":"Let's Encrypt SSL Certificate with Certbot for GoDaddy","text":"<p>Securing your website with HTTPS is a critical step in establishing trust and improving SEO. This guide will show how to generate and install a free Let's Encrypt SSL certificate on a GoDaddy-hosted site using Certbot, and set up automatic certificate renewal.</p> <p></p> <p>This guide will walk you through the steps to generate and install a Let's Encrypt SSL certificate on your GoDaddy-hosted website, including automating certificate renewal.</p>"},{"location":"tech-blog/devops/ssl/#prerequisites","title":"Prerequisites","text":"<ul> <li>A GoDaddy account with VPS or Dedicated Hosting ( ustom SSL certificates can be installed).</li> <li>SSH access to your hosting server. (Optional)</li> <li>Certbot installed on your local machine or server.</li> </ul>"},{"location":"tech-blog/devops/ssl/#step-1-install-certbot","title":"Step 1: Install Certbot","text":"<p>To generate a Let's Encrypt SSL certificate, you'll need to install Certbot. Follow these steps based on your operating system.</p> <p>For Ubuntu/Debian-based systems:</p> <pre><code>sudo apt update\nsudo apt install certbot\n</code></pre> <p>For macOS (using Homebrew):</p> <pre><code>brew install certbot\n</code></pre>"},{"location":"tech-blog/devops/ssl/#step-2-generate-the-ssl-certificate","title":"Step 2: Generate the SSL Certificate","text":"<p>To generate the SSL certificate, use Certbot's DNS validation method. This is particularly useful when you're hosting with GoDaddy but running Certbot elsewhere.</p> <p>Run the following command, replacing yourdomain.com with your domain: <pre><code>certbot certonly --manual --preferred-challenges=dns -d yourdomain.com -d www.yourdomain.com\n</code></pre></p> <p>Certbot will prompt you to create a DNS TXT record to verify domain ownership.</p> <p>Add the DNS TXT record in GoDaddy:</p> <ol> <li>Log in to your GoDaddy account.</li> <li>Go to Domains &gt; DNS Settings.</li> <li>Add a new TXT record with the value provided by Certbot (it will look something like _acme-challenge.yourdomain.com).</li> <li>Wait for DNS propagation to complete (this can take a few minutes to a couple of hours).</li> <li>Confirm the certificate: After DNS propagation, confirm the certificate generation by following the instructions provided by Certbot.</li> </ol> <p>Certbot will generate the following files: - fullchain.pem (certificate) - privkey.pem (private key)</p>"},{"location":"tech-blog/devops/ssl/#step-3-install-the-certificate-on-godaddy","title":"Step 3: Install the Certificate on GoDaddy","text":"<ol> <li>Log in to your GoDaddy cPanel.</li> <li>Go to Security &gt; SSL/TLS.</li> <li>Under Manage SSL Sites, click Manage SSL Sites.</li> <li>Paste the content of:<ul> <li>fullchain.pem into the \"Certificate\" field.</li> <li>privkey.pem into the \"Private Key\" field.</li> </ul> </li> <li>Click Install Certificate to complete the process.</li> </ol>"},{"location":"tech-blog/devops/ssl/#step-4-automate-renewal-with-certbot","title":"Step 4: Automate Renewal with Certbot","text":"<p>Let\u2019s Encrypt certificates expire every 90 days, so it\u2019s important to set up automatic renewal.</p>"},{"location":"tech-blog/devops/ssl/#test-the-renewal-process","title":"Test the renewal process:","text":"<p>Run the following command to test renewal:</p> <pre><code>sudo certbot renew --dry-run\n</code></pre>"},{"location":"tech-blog/devops/ssl/#set-up-a-cron-job-for-automatic-renewal","title":"Set up a cron job for automatic renewal:","text":"<p>To automatically renew your certificates, create a cron job that runs periodically (e.g., every day at midnight): <pre><code>sudo crontab -e\n</code></pre> Add the following line to the cron file:</p> <p><pre><code>0 0 * * * certbot renew --quiet &amp;&amp; systemctl reload apache2  # For Apache\n</code></pre> OR <pre><code>0 0 * * * certbot renew --quiet &amp;&amp; systemctl reload nginx   # For Nginx\n</code></pre></p> <p>This ensures that certificates are checked and renewed daily, if necessary.</p>"},{"location":"tech-blog/devops/ssl/#step-5-verify-ssl-installation","title":"Step 5: Verify SSL Installation","text":"<p>Once the certificate is installed, use the SSL Labs Test to verify your SSL certificate and check for any issues.</p>"},{"location":"tech-blog/devops/ssl/#conclusion","title":"Conclusion","text":"<p>By following this guide, you've successfully secured your GoDaddy-hosted website with a free Let's Encrypt SSL certificate. With automated renewal in place, you can now enjoy secure and hassle-free HTTPS for your site.</p>"},{"location":"tech-blog/devops/test-redis-with-generated-data/","title":"Test Redis with Generated Data","text":"<p>Recently I need to test out Redis integration with a Ruby on Rails application. I wanted to test how fast Redis is with 1GB of data. </p> <p>Instead of manually generating dummy data, I recalled using the Faker library a couple of years ago during my time as a full-stack developer.</p>"},{"location":"tech-blog/devops/test-redis-with-generated-data/#what-is-faker","title":"What is Faker?","text":"<p>Faker is a library that generates fake data such as names, addresses, emails, and phone numbers. It\u2019s a great tool to create realistic data without the need for manually making sample datasets.</p>"},{"location":"tech-blog/devops/test-redis-with-generated-data/#installation","title":"Installation","text":"<p>Faker supports different programming languages. I chose Python this time.</p> <pre><code>pip install faker\n</code></pre>"},{"location":"tech-blog/devops/test-redis-with-generated-data/#code-to-generate-1gb-of-simple-key-value-pair-data","title":"Code to generate 1GB of simple key value pair data","text":"<pre><code>from fake import Faker\nimport csv\n\nfake = Faker()\n\ndef get_text():\n  return ''.join(fake.text(1000)).replace('\\r', '').replace('\\n', '')\n\ndef get_key_and_text_row():\n  return [n, get_text()]\n\nwith open('large.csv', 'w') as csvfile:\n  write = csv.writer(csvfile)\n  for n in range(1, 1000000):\n    writer.writerow(get_key_and_text_row(n))\n</code></pre>"},{"location":"tech-blog/devops/test-redis-with-generated-data/#sample-output","title":"Sample output","text":"<pre><code>1,Interest local deep college particularly and what. Movie bit task matter likely tend.Quality sometimes Democrat join weight minute do. Company event indicate director great pay specific. Continue bill all collection.Help training enter everybody amount. Middle western administration page heart leader similar. Threat once up authority voice give.Customer safe write people. Employee sell throw as so strategy.Drug really control call offer. Effect west service this under program ask contain.Although able region new wear suggest glass. Risk station project including.Because kitchen light enough. Reflect require name democratic wrong without. Begin three fire baby develop federal.Source husband turn attack. Leader win floor young. Member change peace population account high cover. Add but stay crime himself.Sell stand time sure cut decade system.Open fire hair.Leg six event ability other. Nature agree serve affect fund act. Long star improve spring individual.\n2,Station citizen have piece plant. Look science realize stuff television.Single civil poor television actually must. Parent song capital.Too have grow red. Discussion its record you like impact paper.Actually total those main range town. Bit myself future no institution. Suggest unit own at rock personal.Shoulder always today understand especially ball push. Stay appear sort century pull central seven. Site trip memory his start.Member instead option side if result west. Address audience strategy say receive.Candidate later under item yeah will business. Player dog myself purpose.Look trip production water power whether wait rich. Keep sell low if. Quality Republican action main. Sit all see watch same.Lot continue usually wish size. Than morning common hear particular true. Fish minute about.Firm party prepare shoulder main determine health seat. White remain hold card establish interview.Over thousand job minute various adult. Western bill some. Still animal measure.\n3,Church behavior investment protect current every fast reach. Including employee reflect material.Center member relate than war up fear. According discuss pay camera activity home fill soldier.Use table view these pattern popular.Song choose street fish must small. Firm beyond across room other.Ago find traditional listen. Collection learn tell agent newspaper way. Maybe mouth executive country. After future region theory with certainly of.Carry eight spend up.Believe teacher score avoid article. Civil movie eight song across task raise.Plant take painting song. Federal people character animal. Experience fill may.Until someone tree newspaper ball. Rest position spring PM claim. Various animal end parent pick hand great. Help tell hit modern artist involve begin final.Plant certainly father heavy reflect anyone. Parent town make against speak. Agency cover film week plant.Board hope let respond majority sister.\n4,Capital behind tree note whose. Loss condition program. Rest ball including game.View recent matter science red reason choice necessary. Magazine style defense guy special on special. Sell wait per under. Four skin fear yard rather yes responsibility director.Job none player serve service action. Teach phone wait country audience. Especially can source practice. Ground Republican student remember.Grow throughout research understand. Marriage head arm. Share current father successful action future politics.Environment group include executive news model poor worker. Know spend during artist between in morning.Camera easy street side executive note question. Human various someone moment.Require national try continue conference. Property notice structure program fall movie.Small senior big teacher southern class. Point leg social black. How see investment maybe indeed grow.Goal open story tell. Bit fill car think.\n5,We movement pull with near lead. Rock popular when control carry.Treat similar although. Store student five ok bed choice could. Five best community almost.Career these PM provide him. Attorney economic sell behind under wear Congress international. Worry serious social modern law paper.Figure impact employee wall interview major thank serious. Civil protect shake ask. Red hear leader training establish.Dark I role. Avoid defense cut husband culture.Begin account than practice. Table animal it speak.Laugh quickly before eat. Lose about page more. Home new issue.Successful send race road he. Parent must poor box. With wear run.Rock moment allow left system receive decide company. Seven ten then. Better point produce young these study person. American us authority off yes pay none.Our sit clear similar why impact. Could sense when team conference democratic bed mouth. Some whether certainly ago place. Story child matter price thus.\n...\n</code></pre>"},{"location":"tech-blog/devops/test-redis-with-generated-data/#bring-up-a-local-redis-with-docker","title":"Bring up a local Redis with docker","text":"<pre><code>docker run --name my-redis -p 6379:6379 -d redis\n</code></pre>"},{"location":"tech-blog/devops/test-redis-with-generated-data/#load-the-csv-to-docker-redis","title":"Load the csv to docker Redis","text":"<pre><code>docker cp large.csv my-redis:/tmp\ndocker exec -it my-redis -- sh\ncat /tmp/large.csv | awk -F',' '{print \" SET \\\"\"$1\"\\\" \\\"\"$2\"\\\" \\n\"}' | redis-cli --pipe\n</code></pre>"},{"location":"tech-blog/devops/test-redis-with-generated-data/#configuration-in-rails","title":"Configuration in Rails","text":"<p>Setup of Redis is different for different environments. For development, local docker is used. For testing, I use a mock Redis library. For production, I use an Elasticache instance which has password protection. </p> <p>Here is the setup:</p> <pre><code># config/environments/development.rb\n$redis = Redis.new({host: 'localhost', port: 6379, db: 0})\n\n# config/environments/test.rb\n$redis = MockRedis.new\n\n# config/environments/production.rb\n$redis = Redis.new({host: ENV['REDIS_HOST'], port: ENV['REDIS_PORT'], password: ENV['REDIS_PASSWORD'], ssl:true})\n</code></pre>"},{"location":"tech-blog/devops/test-redis-with-generated-data/#sample-rails-controller-and-unit-test","title":"Sample Rails controller and unit test","text":"<p>To keep it simple, I just want to get the value from the cache when the corresponding key is provided.</p> <p>Here is the sample implementation:</p> <pre><code># routes.rb\n\nget '/redis/:id', to: 'redis#index'\n\n\n# redis_controller.rb\n\nclass RedisController &lt; ApplicationController\n\n  def index\n    value = $redis.get(params[:id])\n    if value\n      render json: {key: params[:id], value: value}\n    else\n      render json: {warning: \"value not found from the cache\"}\n    end\n  end\n\nend\n\n# redis_controller_test.rb\n\nrequire \"test_helper\"\nrequire \"mock_redis\"\n\nclass RedisControllerTest &lt; ActionController::TestCase\n\n  setup do\n    $redis.set(1, \"whatever\")\n  end\n\n  test \"should return value if exists\" do\n    get :index, params: {id: 1}, as: :json\n    assert_response 200\n    assert_equal \"{\\\"key\\\":\\\"1\\\",\\\"value\\\":\\\"whatever\\\"}\", @response.body\n  end\n\n  test \"should return warning message if value not exist\" do\n    get :index, params: {id: 2}, as: :json\n    assert_response 200\n    assert_equal \"{\\\"warning\\\":\\\"value not found from the cache\\\"}\", @response.body\n  end\n\nend\n</code></pre>"},{"location":"tech-blog/shell-script/backup-with-rsync/","title":"Backup with rsync","text":"<p>Here are the scripts which I have been using to generate backup for file and mysql database. They  can be run manually but preferably with cron jobs for automation.</p> <p>Note that for the mysql backup script, there is a setting for <code>rotation_day</code>. This is used to keep the number of copy of the database backup, which will be recyled in the final commnad:</p> <p><code>find $backup_path/ -mtime +$rotation_days -exec rm {} \\;</code></p> <p>The backup file is also making use of <code>gpg</code> for encryption.</p>"},{"location":"tech-blog/shell-script/backup-with-rsync/#backup-file-with-rsync","title":"Backup file with rsync","text":"<pre><code>#!/bin/bash\nsource_path=\"[CHANGE ME]\"\ndestination_path=\"[CHANGE ME]\"\n # prefix for log file, eg \"photobackup-\"\nprefix=\"[CHANGE ME]\"\nlogfile=$prefix\"_\"$(date +%d_%m_%Y__%H).log\n\numask 177\n\n # run rsync and exclude the archive folder\n/usr/bin/rsync -avz --log-file=$destination_path/$logfile --exclude archive $source_path $destination_path\n</code></pre>"},{"location":"tech-blog/shell-script/backup-with-rsync/#backup-mysql-database-biweekly-with-encryption","title":"Backup mysql database biweekly with encryption","text":"<pre><code>#!/bin/bash\nuser=\"[CHANGE ME]\"\npassword=\"[CHANGE ME]\"\ngpgrcp=\"[CHANGE ME]\"\nDATABASE=\"[CHANGE ME]\"\n#set daily backup dir\nbackup_path=\"[CHANGE ME]\"\nprefix=\"[CHANGE ME]\"\nsuffix=\"_\"$(date +%d_%m_%Y__%H).sql.gz\nrotation_days=14;\numask 177\n\nFILENAME=$prefix$DATABASE$suffix\n# dump and gzip databases\n/usr/bin/mysqldump -u$user -p$password $DATABASE \\\n           --events --ignore-table=mysql.event | \\\n           /bin/gzip &gt; $backup_path/$FILENAME\n# encrypt files\n/usr/bin/gpg -r $gpgrcp -e $backup_path/$FILENAME\n# delete only gzipped files\n/bin/rm $backup_path/$FILENAME\n\n# recycle files\nfind $backup_path/ -mtime +$rotation_days -exec rm {} \\;\n</code></pre>"},{"location":"tech-blog/shell-script/git-reference/","title":"Git reference","text":""},{"location":"tech-blog/shell-script/git-reference/#using-git-mob","title":"Using Git Mob","text":"<p>Git mob is a command line tool to copilot with your team when you collaborate on code. Read more about Git mob.</p> <p>To setup Git Mob, first you have to definte your information: <pre><code>git config --global user.name \"Janus Chung\"\ngit config --global user.email \"janus.chung@dummy-domain.com\"\n</code></pre></p> <p>Add your teammate to <code>.git-coauthors</code> file <pre><code>$ cat &lt;&lt;-EOF &gt; ~/.git-coauthors\n{\n  \"coauthors\": {\n    \"rh\": {\n      \"name\": \"Robin Hood\",\n      \"email\": \"rhood@dummy-domain.com\"\n    },\n    \"ab\": {\n      \"name\": \"Astro Boy\",\n      \"email\": \"aboy@dummy-domain.com\"\n    }\n  }\n}\nEOF\n</code></pre></p> <p>Say if you want to pair with only Astro Boy <pre><code>git mob ab\n</code></pre></p> <p>If you want to pair with both Robin Hood and Astro Boy <pre><code>git mob rh ab\n</code></pre></p> <p>If you decide to code solo <pre><code>git solo\n</code></pre></p>"},{"location":"tech-blog/shell-script/git-reference/#to-tag-a-build","title":"To tag a build","text":"<pre><code>git checkout main\ngit pull\ngit tag x.x.x\ngit push origin x.x.x\n</code></pre>"},{"location":"tech-blog/shell-script/git-reference/#rebase","title":"Rebase","text":"<pre><code>git checkout main\ngit pull\ngit checkout feature-branch\ngit rebase main feature-branch\ngit push --force origin feature-branch\n</code></pre>"},{"location":"tech-blog/shell-script/git-reference/#bash-helper","title":"Bash helper","text":""},{"location":"tech-blog/shell-script/git-reference/#checkout-main-and-pull","title":"Checkout main and pull","text":"<pre><code>gm(){\n  git checkout main &amp;&amp; git pull\n}\n</code></pre>"},{"location":"tech-blog/shell-script/git-reference/#commit-with-feature-branch-as-the-prefix","title":"Commit with feature branch as the prefix","text":"<pre><code>gcm(){\n  if [[ $# -eq 0 ]] ; then\n    echo \"add a git comment\"\n  else\n    branch=$(git branch | grep '*' | awk '{print $2}')\n    echo $branch\n    echo \"$branch $*\"\n    git commit -m \"$branch $*\"\n  fi    \n}\n</code></pre>"},{"location":"tech-blog/shell-script/git-reference/#push-to-feature-branch-without-typing-it-out","title":"Push to feature branch without typing it out","text":"<pre><code>gp(){\n  branch=$(git branch | grep '*' | awk '{print $2}')\n  git push origin $branch\n}\n</code></pre>"},{"location":"tech-blog/shell-script/git-reference/#auto-tag-and-push-a-new-minor-build","title":"auto tag and push a new minor build","text":"<pre><code>gt(){\n  git checkout main &amp;&amp; git pull\n  LAST_TAG_SHA=$(git show-ref | tail -n 1 | awk '{print $1}')\n  LAST_TAG=$(git show-ref | tail -n 1 | awk '{print $2}' | cut -d '/' -f 3)\n  LAST_TAG_PATCH_VERSION=$(echo \"${LAST_TAG%%-*}\" | cut -d '.' -f 3)\n  NEW_TAG_PATCH_VERSION=$((LAST_TAG_PATCH_VERSION + 1))\n  NEW_TAG=$(echo \"$LAST_TAG\" | cut -d '.' -f 1,2).$NEW_TAG_PATCH_VERSION\n\n  git tag \"$NEW_TAG\" \"$LAST_TAG_SHA\"\n  git push origin \"$NEW_TAG\"\n}\n</code></pre>"},{"location":"tech-blog/shell-script/jq-reference/","title":"Jq reference","text":""},{"location":"tech-blog/shell-script/jq-reference/#read-a-json-file","title":"Read a json file","text":"<p>Give a <code>color.json</code> file with the following content:</p> <pre><code>[\n    {\n        \"name\": \"red\",\n        \"value\": \"#ff0000\"\n    },\n    {\n        \"name\": \"green\",\n        \"value\": \"#008000\"\n    },\n    {\n        \"name\": \"blue\",\n        \"value\": \"#0000ff\"\n    },\n    {\n        \"name\": \"yellow\",\n        \"value\": \"#ffff00\"\n    },\n    {\n        \"name\": \"black\",\n        \"value\": \"#000000\"\n    }\n]\n</code></pre> <p>The following bash script will read the json array one by one</p> <pre><code>#!/usr/bin/env bash\n\nJSON_FILE=\"color.json\"\n\nwhile read -r color; do\n    name=$(jq -r .name &lt;&lt;&lt; \"$color\")\n    value=$(jq -r .value &lt;&lt;&lt; \"$color\")\n    echo \"Color $name has value as $value.\"\ndone &lt; &lt;(jq -c '.[]' $JSON_FILE)\n</code></pre> <p>Here is the sample output:</p> <pre><code>Color red has value as #ff0000.\nColor green has value as #008000.\nColor blue has value as #0000ff.\nColor yellow has value as #ffff00.\nColor black has value as #000000.\n</code></pre>"},{"location":"tech-blog/sql/auto-increment-with-postgres-and-jpa/","title":"Auto Increment with Postgres and JPA","text":""},{"location":"tech-blog/sql/auto-increment-with-postgres-and-jpa/#background","title":"Background","text":"<p>I want to have Postgres to auto generate ID for my Springboot application entity, using Liquibase.</p>"},{"location":"tech-blog/sql/auto-increment-with-postgres-and-jpa/#what-didt-work","title":"What did't work","text":"<p>At first I had the following pair for the Java Entity Class and Liquibase setup: </p> <pre><code>@Id\n@GeneratedValue(strategy = GenerationType.AUTO)\n@Column(name = \"ID\")\nprivate Long id;\n</code></pre> <pre><code>&lt;createTable tableName=\"TICKET\"&gt;\n    &lt;column autoIncrement=\"true\" name=\"ID\" type=\"BIGINT\" remarks=\"Primary key\"&gt;\n        &lt;constraints primaryKey=\"true\" /&gt;\n    &lt;/column&gt;\n</code></pre> <p>which produced the following error:</p> <pre><code>org.postgresql.util.PSQLException: ERROR: relation \"support_system.ticket_seq\" does not exist\n</code></pre> <p>Since it was complainting about the missing sequence, I modified the Liquibase setting to</p> <pre><code>&lt;createTable tableName=\"TICKET\"&gt;\n    &lt;column defaultValueSequenceNext=\"seq_name\" name=\"ID\" type=\"BIGINT\" remarks=\"Primary key\"&gt;\n        &lt;constraints primaryKey=\"true\" /&gt;\n    &lt;/column&gt;\n</code></pre> <p>which brought me to a different bug since I am using a non public schema.</p>"},{"location":"tech-blog/sql/auto-increment-with-postgres-and-jpa/#the-solution","title":"The Solution","text":"<p>After some desperating debugging hours, I consulted my good friend Mark, the SQL guru, who convinced me that I do not need to setup a sequence. Instead, let Postgres generates one for me.</p> <p>Mark's tips</p> <p>You don\u2019t generally need to reference the sequence object explicitly</p> <p>By doing so, I can just create a column using the standard SQL identity, for example, <code>id int generated always as identity</code>. That will create the sequence automatically and automatically call nextval to get a new id value when a new row is inserted.</p> <p>After listened to Mark, I was convinced that I actually did not need to setup a sequence. Therefore, I reversed my chance in Liquibase and continued to try different thing on the JPA side. Here are the setting that finally works:</p> <pre><code>@Id\n@GeneratedValue(strategy = GenerationType.IDENTITY)\n@Column(name = \"ID\")\nprivate Long id;\n</code></pre> <pre><code>&lt;createTable tableName=\"TICKET\"&gt;\n    &lt;column autoIncrement=\"true\" name=\"ID\" type=\"BIGINT\" remarks=\"Primary key\"&gt;\n        &lt;constraints primaryKey=\"true\" /&gt;\n    &lt;/column&gt;\n</code></pre>"},{"location":"tech-blog/sql/auto-increment-with-postgres-and-jpa/#notes","title":"Notes","text":"<p>The PRs which fixed the problem:</p> <ul> <li>Liquibase</li> <li>JPA</li> </ul>"}]}